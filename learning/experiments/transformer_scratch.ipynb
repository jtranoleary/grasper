{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c388af51",
      "metadata": {
        "id": "c388af51"
      },
      "source": [
        "## Loading and Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Kk9SM0DGKF",
        "outputId": "102649a0-d360-48da-c03d-92c795b69373"
      },
      "id": "i-Kk9SM0DGKF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.7.34)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
            "Downloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.11.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (21.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, propcache, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.7.0\n",
            "    Uninstalling fsspec-2025.7.0:\n",
            "      Successfully uninstalled fsspec-2025.7.0\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multiprocess-0.70.16 propcache-0.3.2 xxhash-3.5.0 yarl-1.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7653a0",
      "metadata": {
        "id": "2c7653a0"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import re\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from typing import TypedDict\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193809f0",
      "metadata": {
        "id": "193809f0"
      },
      "outputs": [],
      "source": [
        "class TinyStoriesDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Concatenate all stories into one long list of token IDs\n",
        "        all_token_ids = []\n",
        "        for example in hf_dataset:\n",
        "            text = example['text']\n",
        "            # Add an End-Of-Sequence token to separate stories\n",
        "            token_ids = tokenizer.encode(text) + [tokenizer.eot_token]\n",
        "            all_token_ids.extend(token_ids)\n",
        "\n",
        "        # Create overlapping chunks from the concatenated sequence\n",
        "        for i in range(0, len(all_token_ids) - max_length - 1, stride):\n",
        "            input_chunk = all_token_ids[i : i + max_length]\n",
        "            target_chunk = all_token_ids[i + 1 : i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c7c7b7",
      "metadata": {
        "id": "04c7c7b7"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(hf_dataset_split, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding('gpt2')\n",
        "    dataset = TinyStoriesDataset(hf_dataset_split, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "        drop_last=drop_last, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90b387d",
      "metadata": {
        "id": "f90b387d"
      },
      "outputs": [],
      "source": [
        "class AttentionParams(TypedDict):\n",
        "    W_query: jax.Array\n",
        "    W_key: jax.Array\n",
        "    W_value: jax.Array\n",
        "    W_out: jax.Array\n",
        "\n",
        "class FeedForwardParams(TypedDict):\n",
        "    W_l1: jax.Array\n",
        "    W_l2: jax.Array\n",
        "\n",
        "class LayerNormParams(TypedDict):\n",
        "    W_gamma: jax.Array\n",
        "    W_beta: jax.Array\n",
        "\n",
        "class TransformerBlockParams(TypedDict):\n",
        "    attention: AttentionParams\n",
        "    layer_norm1: LayerNormParams\n",
        "    feed_forward: FeedForwardParams\n",
        "    layer_norm2: LayerNormParams\n",
        "\n",
        "class ModelParams(TypedDict):\n",
        "    embedding: jax.Array\n",
        "    layers: list[TransformerBlockParams]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef08b43c",
      "metadata": {
        "id": "ef08b43c"
      },
      "outputs": [],
      "source": [
        "def init_params(prng_key, vocab_size, d_model, num_layers):\n",
        "    d_ff = d_model * 4\n",
        "    initializer = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    all_params = {}\n",
        "\n",
        "    prng_key, pnrg_embed_key = jax.random.split(prng_key, 2)\n",
        "    all_params['embedding'] = initializer(pnrg_embed_key, (vocab_size, d_model))\n",
        "\n",
        "    all_params['layers'] = []\n",
        "    for _ in range(num_layers):\n",
        "        prng_key, attn_key, ff_key, ln_key = jax.random.split(prng_key, 4)\n",
        "        wq_key, wk_key, wv_key, wo_key = jax.random.split(attn_key, 4)\n",
        "        ff_l1_key, ff_l2_key = jax.random.split(ff_key, 2)\n",
        "        g_key1, b_key1, g_key2, b_key2 = jax.random.split(ln_key, 4)\n",
        "        layer_params: TransformerBlockParams = {\n",
        "            'attention': {\n",
        "                'W_query': initializer(wq_key, (d_model, d_model)),\n",
        "                'W_key': initializer(wk_key, (d_model, d_model)),\n",
        "                'W_value': initializer(wv_key, (d_model, d_model)),\n",
        "                'W_out': initializer(wo_key, (d_model, d_model))\n",
        "            },\n",
        "            'layer_norm1': {\n",
        "                'W_gamma': initializer(g_key1, (1, d_model)),\n",
        "                'W_beta': initializer(b_key2, (1, d_model)),\n",
        "            },\n",
        "            'feed_forward': {\n",
        "                'W_ff_l1': initializer(ff_l1_key, (d_model, d_ff)),\n",
        "                'W_ff_l2': initializer(ff_l2_key, (d_ff, d_model)),\n",
        "            },\n",
        "            'layer_norm2': {\n",
        "                'W_gamma': initializer(g_key2, (1, d_model)),\n",
        "                'W_beta': initializer(b_key2, (1, d_model)),\n",
        "            }\n",
        "\n",
        "        }\n",
        "        all_params['layers'].append(layer_params)\n",
        "\n",
        "    return all_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7acb28",
      "metadata": {
        "id": "1a7acb28"
      },
      "outputs": [],
      "source": [
        "def get_positional_embeddings(seq_len, d_model):\n",
        "    \"\"\"Generates sinusoidal positional embeddings.\"\"\"\n",
        "    positions = jnp.arange(seq_len)[:, jnp.newaxis]\n",
        "    div_term = jnp.exp(jnp.arange(0, d_model, 2) * -(jnp.log(10000.0) / d_model))\n",
        "\n",
        "    pe = jnp.zeros((seq_len, d_model))\n",
        "    pe = pe.at[:, 0::2].set(jnp.sin(positions * div_term))\n",
        "    pe = pe.at[:, 1::2].set(jnp.cos(positions * div_term))\n",
        "\n",
        "    return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f9d06f0",
      "metadata": {
        "id": "8f9d06f0"
      },
      "outputs": [],
      "source": [
        "def dropout(key, rate, x, training=True):\n",
        "    \"\"\"\n",
        "    A simple dropout implementation in JAX.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0.0:\n",
        "        return x\n",
        "\n",
        "    # The keep probability\n",
        "    keep_prob = 1.0 - rate\n",
        "\n",
        "    # 1. Generate a random boolean mask\n",
        "    mask = jax.random.bernoulli(key, keep_prob, x.shape)\n",
        "\n",
        "    # 2. & 3. Apply mask and scale\n",
        "    return jnp.where(mask, x / keep_prob, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13254672",
      "metadata": {
        "id": "13254672"
      },
      "outputs": [],
      "source": [
        "def layer_norm(x, params: LayerNormParams, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Applies Layer Normalization to the input array `x`.\n",
        "\n",
        "    Args:\n",
        "        x (jax.Array): The input array.\n",
        "        gamma (jax.Array): The learnable scale parameter, shape should match the normalization axis.\n",
        "        beta (jax.Array): The learnable bias parameter, shape should match the normalization axis.\n",
        "        epsilon (float): A small value for numerical stability.\n",
        "\n",
        "    Returns:\n",
        "        jax.Array: The normalized output array.\n",
        "    \"\"\"\n",
        "    # Define the axis over which to normalize.\n",
        "    # For a common (batch, sequence, features) setup, this is the last axis.\n",
        "    normalization_axis = -1\n",
        "\n",
        "    # Extract learnable parameters.\n",
        "    gamma = params['W_gamma']\n",
        "    beta = params['W_beta']\n",
        "\n",
        "    # Calculate the mean and variance over the specified axis.\n",
        "    mean = jnp.mean(x, axis=normalization_axis, keepdims=True)\n",
        "    var = jnp.var(x, axis=normalization_axis, keepdims=True, correction=True)\n",
        "\n",
        "    # Normalize the input.\n",
        "    x_norm = (x - mean) / jnp.sqrt(var + epsilon)\n",
        "\n",
        "    # Apply the learnable scale and bias.\n",
        "    output = gamma * x_norm + beta\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82395aa4",
      "metadata": {
        "id": "82395aa4"
      },
      "outputs": [],
      "source": [
        "def multi_head_attention(x, params: AttentionParams, num_heads, head_dim,\n",
        "                        drop_key, drop_rate, training):\n",
        "    \"\"\"\n",
        "    Efficient multi-head causal self-attention.\n",
        "\n",
        "    params: Per-layer params with keys 'W_query', etc.\n",
        "    \"\"\"\n",
        "\n",
        "    # chex.assert_equal(num_heads * head_dim == d_model,\\\n",
        "    #                   ValueError(\"num_heads * head_dim must equal d_model.\"))\n",
        "\n",
        "    # Let these represent: batch, time (seq_len), dimension (d_model).\n",
        "    b, t, d = x.shape\n",
        "\n",
        "    # Calculate Q, K, V from x using respective weight matrices and reshape them\n",
        "    # via number of heads, i.e., partition d_model -> (num_heads x head_dim).\n",
        "    Q = (x @ params['W_query']).reshape((b, t, num_heads, head_dim))\n",
        "    K = (x @ params['W_key']).reshape((b, t, num_heads, head_dim))\n",
        "    V = (x @ params['W_value']).reshape((b, t, num_heads, head_dim))\n",
        "\n",
        "    # Transpose Q, K, V such that we move batch and num_heads to the first two\n",
        "    # axes because we want to parallelize multiplication over these. The\n",
        "    # principal matmul is (seq_len, head_dim) @ same.T such that we get the\n",
        "    # desired (seq_len, seq_len) for attention scores.\n",
        "    # Result: (b, num_heads, t, head_dim)\n",
        "    Q = Q.transpose((0, 2, 1, 3))\n",
        "    K = K.transpose((0, 2, 1, 3))\n",
        "    V = V.transpose((0, 2, 1, 3))\n",
        "\n",
        "    # Calculate attention scores; transpose last 2 axes for matmul compatibility.\n",
        "    attn_scores = Q @ K.transpose((0, 1, 3, 2))\n",
        "\n",
        "    # Generate and apply mask\n",
        "    mask = jnp.triu(jnp.ones((t, t)), k=1)\n",
        "    masked_attn_scores = jnp.where(mask.astype(bool), -jnp.inf, attn_scores)\n",
        "\n",
        "    # Scale and normalize along the horiz axis because rows represent the\n",
        "    # probability distribution per token-query.\n",
        "    attn_weights = jax.nn.softmax(masked_attn_scores / jnp.sqrt(head_dim), axis=-1)\n",
        "    attn_weights = dropout(drop_key, drop_rate, attn_weights, training)\n",
        "\n",
        "    # Matmul attention scores with values\n",
        "    # (b, num_heads t, t) @ (..., t, head_dim) -> (b, nh, t, head_dim), then\n",
        "    # reorder to our original (b, t, nh, hd). Finally reshape to original.\n",
        "    context_vecs = (attn_weights @ V).transpose((0, 2, 1, 3)).reshape((b, t, d))\n",
        "\n",
        "    return context_vecs @ params['W_out']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c706ce",
      "metadata": {
        "id": "b1c706ce"
      },
      "outputs": [],
      "source": [
        "def transformer_block(x, layer_params: TransformerBlockParams, num_heads,\\\n",
        "                      head_dim, key, drop_rate, training):\n",
        "    \"\"\"Applies one layer of multi-head attention and a feed-forward network.\"\"\"\n",
        "    key, attn_key, ffn_key, embed_key = jax.random.split(key, 4)\n",
        "\n",
        "    attn_params = layer_params['attention']\n",
        "    sublayer1_result = multi_head_attention(x, attn_params, num_heads, head_dim,\\\n",
        "                                            attn_key, drop_rate, training)\n",
        "\n",
        "    ln1_params = layer_params['layer_norm1']\n",
        "    sublayer1_result = layer_norm(x + sublayer1_result, ln1_params)\n",
        "\n",
        "    ffn_params = layer_params['feed_forward']\n",
        "    sublayer2_result = jax.nn.relu(sublayer1_result @ ffn_params['W_ff_l1'])\n",
        "    sublayer2_result = sublayer2_result @ ffn_params['W_ff_l2']\n",
        "    sublayer2_result = dropout(ffn_key, drop_rate, sublayer2_result, training)\n",
        "\n",
        "    ln2_params = layer_params['layer_norm2']\n",
        "    return layer_norm(sublayer1_result + sublayer2_result, ln2_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c89438",
      "metadata": {
        "id": "46c89438"
      },
      "outputs": [],
      "source": [
        "def transformer_forward_pass(token_ids: jax.Array, params: ModelParams,\\\n",
        "                             num_heads, drop_key, drop_rate, training):\n",
        "    seq_len, d_model = token_ids.shape[1], params['embedding'].shape[1]\n",
        "    head_dim = d_model // num_heads\n",
        "\n",
        "    # Get word embeddings by selecting tokens from embedding tensor\n",
        "    word_embeds = params['embedding'][token_ids]\n",
        "    pos_embeds = get_positional_embeddings(seq_len, d_model)\n",
        "    x = word_embeds + pos_embeds\n",
        "\n",
        "    # Apply dropout on word embeddings\n",
        "    drop_key, embed_key = jax.random.split(drop_key)\n",
        "    x = dropout(embed_key, drop_rate, x, training)\n",
        "\n",
        "    # Split the key for each layer\n",
        "    layer_keys = jax.random.split(drop_key, len(params['layers']))\n",
        "\n",
        "    for i, layer in enumerate(params['layers']):\n",
        "        x = transformer_block(x, layer, num_heads, head_dim, layer_keys[i],\n",
        "                              drop_rate, training)\n",
        "\n",
        "    logits = x @ params['embedding'].T\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f95dc2",
      "metadata": {
        "id": "c1f95dc2"
      },
      "outputs": [],
      "source": [
        "def generate_text(forward_pass_fn, params, key, start_tokens, max_new_tokens, context_size, num_heads, dropout_rate, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text autoregressively using sampling.\n",
        "    \"\"\"\n",
        "    # Ensure start_tokens is a 2D array: (batch_size, num_tokens)\n",
        "    if start_tokens.ndim == 1:\n",
        "        start_tokens = start_tokens[jnp.newaxis, :]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Get a new key for this generation step\n",
        "        key, step_key = jax.random.split(key)\n",
        "\n",
        "        # Crop context if it's too long\n",
        "        idx_cond = start_tokens[:, -context_size:]\n",
        "\n",
        "        # Call the forward pass function with the correct arguments\n",
        "        logits = forward_pass_fn(\n",
        "            idx_cond, params, num_heads, step_key, dropout_rate, training=False\n",
        "        )\n",
        "\n",
        "        # Get logits for the very last token\n",
        "        last_token_logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply temperature scaling\n",
        "        scaled_logits = last_token_logits / temperature\n",
        "\n",
        "        # Sample from the probability distribution\n",
        "        next_token_id = jax.random.categorical(step_key, last_token_logits)\n",
        "\n",
        "        # Append the new token\n",
        "        start_tokens = jnp.concatenate(\n",
        "            [start_tokens, next_token_id[:, jnp.newaxis]], axis=1\n",
        "        )\n",
        "\n",
        "    return start_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4e1854",
      "metadata": {
        "id": "dc4e1854"
      },
      "outputs": [],
      "source": [
        "def test_pass():\n",
        "    d_model = 256\n",
        "    params = init_params(jax.random.PRNGKey(22), n_vocab, d_model, 1)\n",
        "    num_heads = 8\n",
        "    prng_key = jax.random.PRNGKey(9000)\n",
        "    drop_rate = 0.1\n",
        "    training = False\n",
        "\n",
        "    jit_forward_pass = jax.jit(\n",
        "        transformer_forward_pass, static_argnames=('num_heads', 'drop_rate', 'training')\n",
        "    )\n",
        "    logits = jit_forward_pass(jnp.asarray(inputs), params, num_heads, prng_key,\\\n",
        "                            drop_rate, training)\n",
        "\n",
        "    probs = jax.nn.softmax(logits)\n",
        "\n",
        "    inputs_jax = jnp.asarray(inputs)\n",
        "    targets_jax = jnp.asarray(targets)\n",
        "    inputs_gen = generate_text_simple(jit_forward_pass, params, inputs_jax, 4, 256)\n",
        "\n",
        "    decoded_texts = [dataloader.dataset.tokenizer.decode(seq) for seq in inputs_gen]\n",
        "    decoded_targets = [dataloader.dataset.tokenizer.decode(seq) for seq in targets_jax]\n",
        "    for i, text in enumerate(decoded_texts):\n",
        "        target_text = decoded_targets[i]\n",
        "        print(f\"Target: {target_text}\")\n",
        "        print(f\"Output: {text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d75b52cd",
      "metadata": {
        "id": "d75b52cd"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015c4f51",
      "metadata": {
        "id": "015c4f51"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(logits, targets):\n",
        "    \"\"\"Calculates the cross-entropy loss.\"\"\"\n",
        "    return optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits=logits, labels=targets\n",
        "    ).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66240cef",
      "metadata": {
        "id": "66240cef"
      },
      "outputs": [],
      "source": [
        "def train_step(params, optimizer_state, optimizer, batch_inputs, batch_targets, drop_key, num_heads, drop_rate):\n",
        "    \"\"\"Performs a single training step: loss, gradients, and updates.\"\"\"\n",
        "\n",
        "    def compute_loss(params):\n",
        "        logits = transformer_forward_pass(\n",
        "            batch_inputs, params, num_heads, drop_key, drop_rate, training=True\n",
        "        )\n",
        "        return cross_entropy_loss(logits.reshape(-1, logits.shape[-1]), batch_targets.reshape(-1))\n",
        "\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params)\n",
        "\n",
        "    # The optimizer is now passed in as an argument\n",
        "    updates, optimizer_state = optimizer.update(grads, optimizer_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return params, optimizer_state, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7817fa1",
      "metadata": {
        "id": "c7817fa1"
      },
      "outputs": [],
      "source": [
        "def eval_step(params, batch_inputs, batch_targets, num_heads, drop_rate, key):\n",
        "    \"\"\"Performs a single evaluation step.\"\"\"\n",
        "    logits = transformer_forward_pass(\n",
        "        batch_inputs, params, num_heads, key, drop_rate, training=False\n",
        "    )\n",
        "    return cross_entropy_loss(logits.reshape(-1, logits.shape[-1]), batch_targets.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7180af57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7180af57",
        "outputId": "e4c36772-1670-45d9-96fc-855f0736eebe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-881509372.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m train_dataloader = create_dataloader(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mts_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1346135672.py\u001b[0m in \u001b[0;36mcreate_dataloader\u001b[0;34m(hf_dataset_split, batch_size, max_length, stride, shuffle, drop_last, num_workers)\u001b[0m\n\u001b[1;32m      3\u001b[0m                          num_workers=0):\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTinyStoriesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_dataset_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     dataloader = DataLoader(\n\u001b[1;32m      7\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-503371861.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hf_dataset, tokenizer, max_length, stride)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Add an End-Of-Sequence token to separate stories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meot_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mall_token_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tiktoken/core.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core_bpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_special\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# BPE operates on bytes, but the regex operates on unicode. If we pass a str that is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "ts_dataset = load_dataset(\"roneneldan/TinyStories\")\n",
        "batch_size = 32\n",
        "max_length = 256\n",
        "stride = 128\n",
        "\n",
        "train_dataloader = create_dataloader(\n",
        "    ts_dataset['train'], batch_size=batch_size, max_length=max_length,\n",
        "    stride=stride, shuffle=True, drop_last=True, num_workers=1\n",
        ")\n",
        "val_dataloader = create_dataloader(\n",
        "    ts_dataset['validation'], batch_size=batch_size, max_length=max_length,\n",
        "    stride=stride, shuffle=False, drop_last=True, num_workers=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a94083",
      "metadata": {
        "id": "c9a94083"
      },
      "outputs": [],
      "source": [
        "def do_training(train_dataloader, val_dataloader):\n",
        "    # --- 1. Hyperparameters ---\n",
        "    d_model = 256\n",
        "    num_layers = 6\n",
        "    num_heads = 8\n",
        "    dropout_rate = 0.1\n",
        "    batch_size = 32\n",
        "    max_length = 256\n",
        "    stride = 128\n",
        "    learning_rate = 1e-4\n",
        "    num_epochs = 5\n",
        "\n",
        "    params ={}\n",
        "    optimizer_state=None\n",
        "\n",
        "    path = ocp.test_utils.erase_and_create_empty('./model_checkpoints')\n",
        "    checkpointer = ocp.AsyncCheckpointer(ocp.StandardCheckpointHandler())\n",
        "    save_data = {'params': params, 'optimizer_state': optimizer_state}\n",
        "\n",
        "    # Get vocab size from the tokenizer used in the dataloader\n",
        "    tokenizer = tiktoken.get_encoding('gpt2')\n",
        "    vocab_size = tokenizer.n_vocab\n",
        "\n",
        "    # --- 3. Initialize Model and Optimizer ---\n",
        "    main_key = jax.random.PRNGKey(0)\n",
        "    init_key, train_key = jax.random.split(main_key)\n",
        "\n",
        "    print(\"Initializing params...\")\n",
        "\n",
        "    params = init_params(init_key, vocab_size, d_model, num_layers)\n",
        "    optimizer = optax.adam(learning_rate)\n",
        "    optimizer_state = optimizer.init(params)\n",
        "\n",
        "    jit_train_step = jax.jit(\n",
        "        train_step, static_argnames=('optimizer', 'num_heads', 'drop_rate')\n",
        "    )\n",
        "    jit_eval_step = jax.jit(\n",
        "        eval_step, static_argnames=('num_heads', 'drop_rate')\n",
        "    )\n",
        "\n",
        "    print(\"Begin training loop.\")\n",
        "\n",
        "    # --- 4. The Training Loop ---\n",
        "    for epoch in range(num_epochs):\n",
        "        # --- Training Phase ---\n",
        "        total_train_loss = 0\n",
        "        for i, (inputs, targets) in enumerate(train_dataloader):\n",
        "            train_key, step_key = jax.random.split(train_key)\n",
        "\n",
        "            inputs_jax = jnp.asarray(inputs)\n",
        "            targets_jax = jnp.asarray(targets)\n",
        "\n",
        "            # Call the JIT-compiled function with the optimizer\n",
        "            params, optimizer_state, loss = jit_train_step(\n",
        "                params, optimizer_state, optimizer, inputs_jax, targets_jax,\n",
        "                step_key, num_heads, dropout_rate\n",
        "            )\n",
        "\n",
        "            total_train_loss += loss\n",
        "            if (i + 1) % 50 == 0:\n",
        "                print(f\"--- Epoch {epoch+1}/{num_epochs} | Batch {i+1}/{len(train_dataloader)} | Loss: {loss:.4f}\")\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        total_val_loss = 0\n",
        "        for inputs, targets in val_dataloader:\n",
        "            # We don't need a new key for validation as dropout is disabled\n",
        "            inputs_jax, targets_jax = jnp.asarray(inputs), jnp.asarray(targets)\n",
        "            loss = jit_eval_step(\n",
        "                params, inputs_jax, targets_jax, num_heads, dropout_rate, main_key\n",
        "            )\n",
        "            total_val_loss += loss\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "        print(f\"--- End of Epoch {epoch+1} | Avg Train Loss: {avg_train_loss:.4f} | Avg Val Loss: {avg_val_loss:.4f} ---\")\n",
        "\n",
        "        # --- Generate and print a sample at the end of each epoch ---\n",
        "        train_key, gen_key = jax.random.split(train_key)\n",
        "        start_text = \"as they turned around\"\n",
        "        start_tokens = jnp.asarray(\n",
        "            train_dataloader.dataset.tokenizer.encode(start_text)\n",
        "        )\n",
        "\n",
        "        generated_tokens = generate_text(\n",
        "            transformer_forward_pass, params, gen_key, start_tokens,\n",
        "            max_new_tokens=32, context_size=max_length,\n",
        "            num_heads=num_heads, dropout_rate=dropout_rate, temperature=0.5\n",
        "        )\n",
        "\n",
        "        # Decode the entire generated sequence\n",
        "        decoded_text = train_dataloader.dataset.tokenizer.decode(generated_tokens[0].tolist())\n",
        "        print(f\"Sample: {decoded_text}\\n\")\n",
        "\n",
        "        checkpointer.save(path / '1', args=ocp.args.StandardSave(save_data))\n",
        "        print(f\"Checkpoint for epoch {epoch} saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a576ec7d",
      "metadata": {
        "id": "a576ec7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba1071bc-83c9-47de-95f4-a39a4a9dd7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing params...\n",
            "Begin training loop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Epoch 1/5 | Batch 50/231441 | Loss: 10.4633\n",
            "--- Epoch 1/5 | Batch 100/231441 | Loss: 10.0181\n",
            "--- Epoch 1/5 | Batch 150/231441 | Loss: 9.5122\n",
            "--- Epoch 1/5 | Batch 200/231441 | Loss: 9.0073\n",
            "--- Epoch 1/5 | Batch 250/231441 | Loss: 8.3923\n",
            "--- Epoch 1/5 | Batch 300/231441 | Loss: 7.8657\n",
            "--- Epoch 1/5 | Batch 350/231441 | Loss: 7.3888\n",
            "--- Epoch 1/5 | Batch 400/231441 | Loss: 6.9569\n",
            "--- Epoch 1/5 | Batch 450/231441 | Loss: 6.6737\n",
            "--- Epoch 1/5 | Batch 500/231441 | Loss: 6.4646\n",
            "--- Epoch 1/5 | Batch 550/231441 | Loss: 6.3255\n",
            "--- Epoch 1/5 | Batch 600/231441 | Loss: 6.1050\n",
            "--- Epoch 1/5 | Batch 650/231441 | Loss: 6.0654\n",
            "--- Epoch 1/5 | Batch 700/231441 | Loss: 6.0761\n",
            "--- Epoch 1/5 | Batch 750/231441 | Loss: 5.9723\n",
            "--- Epoch 1/5 | Batch 800/231441 | Loss: 5.9577\n",
            "--- Epoch 1/5 | Batch 850/231441 | Loss: 5.9065\n",
            "--- Epoch 1/5 | Batch 900/231441 | Loss: 6.0005\n",
            "--- Epoch 1/5 | Batch 950/231441 | Loss: 6.0497\n",
            "--- Epoch 1/5 | Batch 1000/231441 | Loss: 5.9992\n",
            "--- Epoch 1/5 | Batch 1050/231441 | Loss: 5.9868\n",
            "--- Epoch 1/5 | Batch 1100/231441 | Loss: 5.8999\n",
            "--- Epoch 1/5 | Batch 1150/231441 | Loss: 6.1299\n",
            "--- Epoch 1/5 | Batch 1200/231441 | Loss: 6.0187\n",
            "--- Epoch 1/5 | Batch 1250/231441 | Loss: 5.8998\n",
            "--- Epoch 1/5 | Batch 1300/231441 | Loss: 5.9936\n",
            "--- Epoch 1/5 | Batch 1350/231441 | Loss: 5.8912\n",
            "--- Epoch 1/5 | Batch 1400/231441 | Loss: 5.9902\n",
            "--- Epoch 1/5 | Batch 1450/231441 | Loss: 6.0012\n",
            "--- Epoch 1/5 | Batch 1500/231441 | Loss: 5.9950\n",
            "--- Epoch 1/5 | Batch 1550/231441 | Loss: 6.0199\n",
            "--- Epoch 1/5 | Batch 1600/231441 | Loss: 5.9033\n",
            "--- Epoch 1/5 | Batch 1650/231441 | Loss: 6.0514\n",
            "--- Epoch 1/5 | Batch 1700/231441 | Loss: 6.0579\n",
            "--- Epoch 1/5 | Batch 1750/231441 | Loss: 5.8881\n",
            "--- Epoch 1/5 | Batch 1800/231441 | Loss: 5.9075\n",
            "--- Epoch 1/5 | Batch 1850/231441 | Loss: 6.0620\n",
            "--- Epoch 1/5 | Batch 1900/231441 | Loss: 5.9529\n",
            "--- Epoch 1/5 | Batch 1950/231441 | Loss: 6.0052\n",
            "--- Epoch 1/5 | Batch 2000/231441 | Loss: 5.9319\n",
            "--- Epoch 1/5 | Batch 2050/231441 | Loss: 5.9229\n",
            "--- Epoch 1/5 | Batch 2100/231441 | Loss: 5.9601\n",
            "--- Epoch 1/5 | Batch 2150/231441 | Loss: 5.9514\n",
            "--- Epoch 1/5 | Batch 2200/231441 | Loss: 5.9408\n",
            "--- Epoch 1/5 | Batch 2250/231441 | Loss: 5.9352\n",
            "--- Epoch 1/5 | Batch 2300/231441 | Loss: 5.9637\n",
            "--- Epoch 1/5 | Batch 2350/231441 | Loss: 5.9926\n",
            "--- Epoch 1/5 | Batch 2400/231441 | Loss: 5.9986\n",
            "--- Epoch 1/5 | Batch 2450/231441 | Loss: 5.9406\n",
            "--- Epoch 1/5 | Batch 2500/231441 | Loss: 5.9890\n",
            "--- Epoch 1/5 | Batch 2550/231441 | Loss: 5.9490\n",
            "--- Epoch 1/5 | Batch 2600/231441 | Loss: 6.0053\n",
            "--- Epoch 1/5 | Batch 2650/231441 | Loss: 5.9744\n",
            "--- Epoch 1/5 | Batch 2700/231441 | Loss: 5.9766\n",
            "--- Epoch 1/5 | Batch 2750/231441 | Loss: 5.9246\n",
            "--- Epoch 1/5 | Batch 2800/231441 | Loss: 5.9701\n",
            "--- Epoch 1/5 | Batch 2850/231441 | Loss: 5.9823\n",
            "--- Epoch 1/5 | Batch 2900/231441 | Loss: 5.9639\n",
            "--- Epoch 1/5 | Batch 2950/231441 | Loss: 6.0443\n",
            "--- Epoch 1/5 | Batch 3000/231441 | Loss: 5.9580\n",
            "--- Epoch 1/5 | Batch 3050/231441 | Loss: 5.8877\n",
            "--- Epoch 1/5 | Batch 3100/231441 | Loss: 5.8699\n",
            "--- Epoch 1/5 | Batch 3150/231441 | Loss: 5.7761\n",
            "--- Epoch 1/5 | Batch 3200/231441 | Loss: 5.6707\n",
            "--- Epoch 1/5 | Batch 3250/231441 | Loss: 5.7604\n",
            "--- Epoch 1/5 | Batch 3300/231441 | Loss: 5.7717\n",
            "--- Epoch 1/5 | Batch 3350/231441 | Loss: 5.6256\n",
            "--- Epoch 1/5 | Batch 3400/231441 | Loss: 5.5358\n",
            "--- Epoch 1/5 | Batch 3450/231441 | Loss: 5.6106\n",
            "--- Epoch 1/5 | Batch 3500/231441 | Loss: 5.4453\n",
            "--- Epoch 1/5 | Batch 3550/231441 | Loss: 5.4813\n",
            "--- Epoch 1/5 | Batch 3600/231441 | Loss: 5.3389\n",
            "--- Epoch 1/5 | Batch 3650/231441 | Loss: 5.2562\n",
            "--- Epoch 1/5 | Batch 3700/231441 | Loss: 5.2331\n",
            "--- Epoch 1/5 | Batch 3750/231441 | Loss: 5.2861\n",
            "--- Epoch 1/5 | Batch 3800/231441 | Loss: 5.2382\n",
            "--- Epoch 1/5 | Batch 3850/231441 | Loss: 5.2297\n",
            "--- Epoch 1/5 | Batch 3900/231441 | Loss: 5.3103\n",
            "--- Epoch 1/5 | Batch 3950/231441 | Loss: 5.1106\n",
            "--- Epoch 1/5 | Batch 4000/231441 | Loss: 5.1376\n",
            "--- Epoch 1/5 | Batch 4050/231441 | Loss: 4.9992\n",
            "--- Epoch 1/5 | Batch 4100/231441 | Loss: 5.0721\n",
            "--- Epoch 1/5 | Batch 4150/231441 | Loss: 5.1472\n",
            "--- Epoch 1/5 | Batch 4200/231441 | Loss: 5.1352\n",
            "--- Epoch 1/5 | Batch 4250/231441 | Loss: 5.1557\n",
            "--- Epoch 1/5 | Batch 4300/231441 | Loss: 4.8901\n",
            "--- Epoch 1/5 | Batch 4350/231441 | Loss: 5.0817\n",
            "--- Epoch 1/5 | Batch 4400/231441 | Loss: 5.0295\n",
            "--- Epoch 1/5 | Batch 4450/231441 | Loss: 5.0982\n",
            "--- Epoch 1/5 | Batch 4500/231441 | Loss: 5.1094\n",
            "--- Epoch 1/5 | Batch 4550/231441 | Loss: 4.9722\n",
            "--- Epoch 1/5 | Batch 4600/231441 | Loss: 5.0313\n",
            "--- Epoch 1/5 | Batch 4650/231441 | Loss: 5.0819\n",
            "--- Epoch 1/5 | Batch 4700/231441 | Loss: 4.9783\n",
            "--- Epoch 1/5 | Batch 4750/231441 | Loss: 4.9540\n",
            "--- Epoch 1/5 | Batch 4800/231441 | Loss: 4.9703\n",
            "--- Epoch 1/5 | Batch 4850/231441 | Loss: 5.0810\n",
            "--- Epoch 1/5 | Batch 4900/231441 | Loss: 4.9197\n",
            "--- Epoch 1/5 | Batch 4950/231441 | Loss: 5.0613\n",
            "--- Epoch 1/5 | Batch 5000/231441 | Loss: 4.8109\n",
            "--- Epoch 1/5 | Batch 5050/231441 | Loss: 4.9067\n",
            "--- Epoch 1/5 | Batch 5100/231441 | Loss: 4.8132\n",
            "--- Epoch 1/5 | Batch 5150/231441 | Loss: 4.8186\n",
            "--- Epoch 1/5 | Batch 5200/231441 | Loss: 4.7053\n",
            "--- Epoch 1/5 | Batch 5250/231441 | Loss: 4.9796\n",
            "--- Epoch 1/5 | Batch 5300/231441 | Loss: 4.7982\n",
            "--- Epoch 1/5 | Batch 5350/231441 | Loss: 4.6848\n",
            "--- Epoch 1/5 | Batch 5400/231441 | Loss: 4.6638\n",
            "--- Epoch 1/5 | Batch 5450/231441 | Loss: 4.8130\n",
            "--- Epoch 1/5 | Batch 5500/231441 | Loss: 4.7321\n",
            "--- Epoch 1/5 | Batch 5550/231441 | Loss: 4.7472\n",
            "--- Epoch 1/5 | Batch 5600/231441 | Loss: 4.6356\n",
            "--- Epoch 1/5 | Batch 5650/231441 | Loss: 4.6167\n",
            "--- Epoch 1/5 | Batch 5700/231441 | Loss: 4.6316\n",
            "--- Epoch 1/5 | Batch 5750/231441 | Loss: 4.5961\n",
            "--- Epoch 1/5 | Batch 5800/231441 | Loss: 4.5545\n",
            "--- Epoch 1/5 | Batch 5850/231441 | Loss: 4.5513\n",
            "--- Epoch 1/5 | Batch 5900/231441 | Loss: 4.6688\n",
            "--- Epoch 1/5 | Batch 5950/231441 | Loss: 4.5380\n",
            "--- Epoch 1/5 | Batch 6000/231441 | Loss: 4.5214\n",
            "--- Epoch 1/5 | Batch 6050/231441 | Loss: 4.4819\n",
            "--- Epoch 1/5 | Batch 6100/231441 | Loss: 4.6098\n",
            "--- Epoch 1/5 | Batch 6150/231441 | Loss: 4.7217\n",
            "--- Epoch 1/5 | Batch 6200/231441 | Loss: 4.6602\n",
            "--- Epoch 1/5 | Batch 6250/231441 | Loss: 4.6509\n",
            "--- Epoch 1/5 | Batch 6300/231441 | Loss: 4.5092\n",
            "--- Epoch 1/5 | Batch 6350/231441 | Loss: 4.7066\n",
            "--- Epoch 1/5 | Batch 6400/231441 | Loss: 4.4964\n",
            "--- Epoch 1/5 | Batch 6450/231441 | Loss: 4.4891\n",
            "--- Epoch 1/5 | Batch 6500/231441 | Loss: 4.3668\n",
            "--- Epoch 1/5 | Batch 6550/231441 | Loss: 4.5002\n",
            "--- Epoch 1/5 | Batch 6600/231441 | Loss: 4.5201\n",
            "--- Epoch 1/5 | Batch 6650/231441 | Loss: 4.4931\n",
            "--- Epoch 1/5 | Batch 6700/231441 | Loss: 4.5030\n",
            "--- Epoch 1/5 | Batch 6750/231441 | Loss: 4.4820\n",
            "--- Epoch 1/5 | Batch 6800/231441 | Loss: 4.4619\n",
            "--- Epoch 1/5 | Batch 6850/231441 | Loss: 4.3197\n",
            "--- Epoch 1/5 | Batch 6900/231441 | Loss: 4.3157\n",
            "--- Epoch 1/5 | Batch 6950/231441 | Loss: 4.4213\n",
            "--- Epoch 1/5 | Batch 7000/231441 | Loss: 4.4814\n",
            "--- Epoch 1/5 | Batch 7050/231441 | Loss: 4.4536\n",
            "--- Epoch 1/5 | Batch 7100/231441 | Loss: 4.2827\n",
            "--- Epoch 1/5 | Batch 7150/231441 | Loss: 4.4543\n",
            "--- Epoch 1/5 | Batch 7200/231441 | Loss: 4.3015\n",
            "--- Epoch 1/5 | Batch 7250/231441 | Loss: 4.4494\n",
            "--- Epoch 1/5 | Batch 7300/231441 | Loss: 4.3988\n",
            "--- Epoch 1/5 | Batch 7350/231441 | Loss: 4.2925\n",
            "--- Epoch 1/5 | Batch 7400/231441 | Loss: 4.4512\n",
            "--- Epoch 1/5 | Batch 7450/231441 | Loss: 4.4124\n",
            "--- Epoch 1/5 | Batch 7500/231441 | Loss: 4.3864\n",
            "--- Epoch 1/5 | Batch 7550/231441 | Loss: 4.2246\n",
            "--- Epoch 1/5 | Batch 7600/231441 | Loss: 4.1525\n",
            "--- Epoch 1/5 | Batch 7650/231441 | Loss: 4.3329\n",
            "--- Epoch 1/5 | Batch 7700/231441 | Loss: 4.2448\n",
            "--- Epoch 1/5 | Batch 7750/231441 | Loss: 4.1530\n",
            "--- Epoch 1/5 | Batch 7800/231441 | Loss: 4.1251\n",
            "--- Epoch 1/5 | Batch 7850/231441 | Loss: 4.1196\n",
            "--- Epoch 1/5 | Batch 7900/231441 | Loss: 4.2883\n",
            "--- Epoch 1/5 | Batch 7950/231441 | Loss: 4.2511\n",
            "--- Epoch 1/5 | Batch 8000/231441 | Loss: 4.1789\n",
            "--- Epoch 1/5 | Batch 8050/231441 | Loss: 4.0704\n",
            "--- Epoch 1/5 | Batch 8100/231441 | Loss: 4.1364\n",
            "--- Epoch 1/5 | Batch 8150/231441 | Loss: 4.1580\n",
            "--- Epoch 1/5 | Batch 8200/231441 | Loss: 4.1849\n",
            "--- Epoch 1/5 | Batch 8250/231441 | Loss: 4.0705\n",
            "--- Epoch 1/5 | Batch 8300/231441 | Loss: 4.2001\n",
            "--- Epoch 1/5 | Batch 8350/231441 | Loss: 4.0279\n",
            "--- Epoch 1/5 | Batch 8400/231441 | Loss: 4.0998\n",
            "--- Epoch 1/5 | Batch 8450/231441 | Loss: 3.9665\n",
            "--- Epoch 1/5 | Batch 8500/231441 | Loss: 4.1435\n",
            "--- Epoch 1/5 | Batch 8550/231441 | Loss: 3.9625\n",
            "--- Epoch 1/5 | Batch 8600/231441 | Loss: 4.0289\n",
            "--- Epoch 1/5 | Batch 8650/231441 | Loss: 3.9396\n",
            "--- Epoch 1/5 | Batch 8700/231441 | Loss: 4.0733\n",
            "--- Epoch 1/5 | Batch 8750/231441 | Loss: 4.0117\n",
            "--- Epoch 1/5 | Batch 8800/231441 | Loss: 3.8649\n",
            "--- Epoch 1/5 | Batch 8850/231441 | Loss: 3.9374\n",
            "--- Epoch 1/5 | Batch 8900/231441 | Loss: 4.0019\n",
            "--- Epoch 1/5 | Batch 8950/231441 | Loss: 3.9847\n",
            "--- Epoch 1/5 | Batch 9000/231441 | Loss: 4.0098\n",
            "--- Epoch 1/5 | Batch 9050/231441 | Loss: 3.8442\n",
            "--- Epoch 1/5 | Batch 9100/231441 | Loss: 3.9831\n",
            "--- Epoch 1/5 | Batch 9150/231441 | Loss: 3.8599\n",
            "--- Epoch 1/5 | Batch 9200/231441 | Loss: 3.9988\n",
            "--- Epoch 1/5 | Batch 9250/231441 | Loss: 3.8218\n",
            "--- Epoch 1/5 | Batch 9300/231441 | Loss: 3.7348\n",
            "--- Epoch 1/5 | Batch 9350/231441 | Loss: 3.9596\n",
            "--- Epoch 1/5 | Batch 9400/231441 | Loss: 3.8233\n",
            "--- Epoch 1/5 | Batch 9450/231441 | Loss: 3.8260\n",
            "--- Epoch 1/5 | Batch 9500/231441 | Loss: 4.0065\n",
            "--- Epoch 1/5 | Batch 9550/231441 | Loss: 3.8753\n",
            "--- Epoch 1/5 | Batch 9600/231441 | Loss: 3.8582\n",
            "--- Epoch 1/5 | Batch 9650/231441 | Loss: 3.8165\n",
            "--- Epoch 1/5 | Batch 9700/231441 | Loss: 3.7980\n",
            "--- Epoch 1/5 | Batch 9750/231441 | Loss: 3.6611\n",
            "--- Epoch 1/5 | Batch 9800/231441 | Loss: 3.8684\n",
            "--- Epoch 1/5 | Batch 9850/231441 | Loss: 3.7357\n",
            "--- Epoch 1/5 | Batch 9900/231441 | Loss: 3.6836\n",
            "--- Epoch 1/5 | Batch 9950/231441 | Loss: 3.7850\n",
            "--- Epoch 1/5 | Batch 10000/231441 | Loss: 3.8010\n",
            "--- Epoch 1/5 | Batch 10050/231441 | Loss: 3.6727\n",
            "--- Epoch 1/5 | Batch 10100/231441 | Loss: 3.8658\n",
            "--- Epoch 1/5 | Batch 10150/231441 | Loss: 3.8141\n",
            "--- Epoch 1/5 | Batch 10200/231441 | Loss: 3.6956\n",
            "--- Epoch 1/5 | Batch 10250/231441 | Loss: 3.7283\n",
            "--- Epoch 1/5 | Batch 10300/231441 | Loss: 3.5787\n",
            "--- Epoch 1/5 | Batch 10350/231441 | Loss: 3.8075\n",
            "--- Epoch 1/5 | Batch 10400/231441 | Loss: 3.6933\n",
            "--- Epoch 1/5 | Batch 10450/231441 | Loss: 3.8758\n",
            "--- Epoch 1/5 | Batch 10500/231441 | Loss: 3.7391\n",
            "--- Epoch 1/5 | Batch 10550/231441 | Loss: 3.7690\n",
            "--- Epoch 1/5 | Batch 10600/231441 | Loss: 3.9385\n",
            "--- Epoch 1/5 | Batch 10650/231441 | Loss: 3.7369\n",
            "--- Epoch 1/5 | Batch 10700/231441 | Loss: 3.6771\n",
            "--- Epoch 1/5 | Batch 10750/231441 | Loss: 3.6211\n",
            "--- Epoch 1/5 | Batch 10800/231441 | Loss: 3.7469\n",
            "--- Epoch 1/5 | Batch 10850/231441 | Loss: 3.5861\n",
            "--- Epoch 1/5 | Batch 10900/231441 | Loss: 3.7149\n",
            "--- Epoch 1/5 | Batch 10950/231441 | Loss: 3.6632\n",
            "--- Epoch 1/5 | Batch 11000/231441 | Loss: 3.6158\n",
            "--- Epoch 1/5 | Batch 11050/231441 | Loss: 3.8597\n",
            "--- Epoch 1/5 | Batch 11100/231441 | Loss: 3.6770\n",
            "--- Epoch 1/5 | Batch 11150/231441 | Loss: 3.4986\n",
            "--- Epoch 1/5 | Batch 11200/231441 | Loss: 3.5644\n",
            "--- Epoch 1/5 | Batch 11250/231441 | Loss: 3.3304\n",
            "--- Epoch 1/5 | Batch 11300/231441 | Loss: 3.7914\n",
            "--- Epoch 1/5 | Batch 11350/231441 | Loss: 3.6914\n",
            "--- Epoch 1/5 | Batch 11400/231441 | Loss: 3.6740\n",
            "--- Epoch 1/5 | Batch 11450/231441 | Loss: 3.5926\n",
            "--- Epoch 1/5 | Batch 11500/231441 | Loss: 3.7895\n",
            "--- Epoch 1/5 | Batch 11550/231441 | Loss: 3.5269\n",
            "--- Epoch 1/5 | Batch 11600/231441 | Loss: 3.6308\n",
            "--- Epoch 1/5 | Batch 11650/231441 | Loss: 3.6677\n",
            "--- Epoch 1/5 | Batch 11700/231441 | Loss: 3.6242\n",
            "--- Epoch 1/5 | Batch 11750/231441 | Loss: 3.5697\n",
            "--- Epoch 1/5 | Batch 11800/231441 | Loss: 3.6444\n",
            "--- Epoch 1/5 | Batch 11850/231441 | Loss: 3.5600\n",
            "--- Epoch 1/5 | Batch 11900/231441 | Loss: 3.6017\n",
            "--- Epoch 1/5 | Batch 11950/231441 | Loss: 3.8627\n",
            "--- Epoch 1/5 | Batch 12000/231441 | Loss: 3.6116\n",
            "--- Epoch 1/5 | Batch 12050/231441 | Loss: 3.6650\n",
            "--- Epoch 1/5 | Batch 12100/231441 | Loss: 3.6490\n",
            "--- Epoch 1/5 | Batch 12150/231441 | Loss: 3.5962\n",
            "--- Epoch 1/5 | Batch 12200/231441 | Loss: 3.7195\n",
            "--- Epoch 1/5 | Batch 12250/231441 | Loss: 3.5483\n",
            "--- Epoch 1/5 | Batch 12300/231441 | Loss: 3.6784\n",
            "--- Epoch 1/5 | Batch 12350/231441 | Loss: 3.4404\n",
            "--- Epoch 1/5 | Batch 12400/231441 | Loss: 3.5106\n",
            "--- Epoch 1/5 | Batch 12450/231441 | Loss: 3.5764\n",
            "--- Epoch 1/5 | Batch 12500/231441 | Loss: 3.5157\n",
            "--- Epoch 1/5 | Batch 12550/231441 | Loss: 3.5487\n",
            "--- Epoch 1/5 | Batch 12600/231441 | Loss: 3.5514\n",
            "--- Epoch 1/5 | Batch 12650/231441 | Loss: 3.6591\n",
            "--- Epoch 1/5 | Batch 12700/231441 | Loss: 3.7023\n",
            "--- Epoch 1/5 | Batch 12750/231441 | Loss: 3.3052\n",
            "--- Epoch 1/5 | Batch 12800/231441 | Loss: 3.4385\n",
            "--- Epoch 1/5 | Batch 12850/231441 | Loss: 3.6350\n",
            "--- Epoch 1/5 | Batch 12900/231441 | Loss: 3.6547\n",
            "--- Epoch 1/5 | Batch 12950/231441 | Loss: 3.5129\n",
            "--- Epoch 1/5 | Batch 13000/231441 | Loss: 3.4749\n",
            "--- Epoch 1/5 | Batch 13050/231441 | Loss: 3.5100\n",
            "--- Epoch 1/5 | Batch 13100/231441 | Loss: 3.5526\n",
            "--- Epoch 1/5 | Batch 13150/231441 | Loss: 3.5289\n",
            "--- Epoch 1/5 | Batch 13200/231441 | Loss: 3.3807\n",
            "--- Epoch 1/5 | Batch 13250/231441 | Loss: 3.6423\n",
            "--- Epoch 1/5 | Batch 13300/231441 | Loss: 3.6369\n",
            "--- Epoch 1/5 | Batch 13350/231441 | Loss: 3.4810\n",
            "--- Epoch 1/5 | Batch 13400/231441 | Loss: 3.2839\n",
            "--- Epoch 1/5 | Batch 13450/231441 | Loss: 3.3782\n",
            "--- Epoch 1/5 | Batch 13500/231441 | Loss: 3.5003\n",
            "--- Epoch 1/5 | Batch 13550/231441 | Loss: 3.4062\n",
            "--- Epoch 1/5 | Batch 13600/231441 | Loss: 3.5604\n",
            "--- Epoch 1/5 | Batch 13650/231441 | Loss: 3.4912\n",
            "--- Epoch 1/5 | Batch 13700/231441 | Loss: 3.2907\n",
            "--- Epoch 1/5 | Batch 13750/231441 | Loss: 3.5013\n",
            "--- Epoch 1/5 | Batch 13800/231441 | Loss: 3.4689\n",
            "--- Epoch 1/5 | Batch 13850/231441 | Loss: 3.3702\n",
            "--- Epoch 1/5 | Batch 13900/231441 | Loss: 3.3413\n",
            "--- Epoch 1/5 | Batch 13950/231441 | Loss: 3.2987\n",
            "--- Epoch 1/5 | Batch 14000/231441 | Loss: 3.4735\n",
            "--- Epoch 1/5 | Batch 14050/231441 | Loss: 3.1939\n",
            "--- Epoch 1/5 | Batch 14100/231441 | Loss: 3.3977\n",
            "--- Epoch 1/5 | Batch 14150/231441 | Loss: 3.6637\n",
            "--- Epoch 1/5 | Batch 14200/231441 | Loss: 3.5386\n",
            "--- Epoch 1/5 | Batch 14250/231441 | Loss: 3.4436\n",
            "--- Epoch 1/5 | Batch 14300/231441 | Loss: 3.2903\n",
            "--- Epoch 1/5 | Batch 14350/231441 | Loss: 3.5319\n",
            "--- Epoch 1/5 | Batch 14400/231441 | Loss: 3.3195\n",
            "--- Epoch 1/5 | Batch 14450/231441 | Loss: 3.4223\n",
            "--- Epoch 1/5 | Batch 14500/231441 | Loss: 3.3953\n",
            "--- Epoch 1/5 | Batch 14550/231441 | Loss: 3.3562\n",
            "--- Epoch 1/5 | Batch 14600/231441 | Loss: 3.4694\n",
            "--- Epoch 1/5 | Batch 14650/231441 | Loss: 3.4057\n",
            "--- Epoch 1/5 | Batch 14700/231441 | Loss: 3.4049\n",
            "--- Epoch 1/5 | Batch 14750/231441 | Loss: 3.3158\n",
            "--- Epoch 1/5 | Batch 14800/231441 | Loss: 3.3624\n",
            "--- Epoch 1/5 | Batch 14850/231441 | Loss: 3.3865\n",
            "--- Epoch 1/5 | Batch 14900/231441 | Loss: 3.3082\n",
            "--- Epoch 1/5 | Batch 14950/231441 | Loss: 3.4946\n",
            "--- Epoch 1/5 | Batch 15000/231441 | Loss: 3.2383\n",
            "--- Epoch 1/5 | Batch 15050/231441 | Loss: 3.3561\n",
            "--- Epoch 1/5 | Batch 15100/231441 | Loss: 3.3574\n",
            "--- Epoch 1/5 | Batch 15150/231441 | Loss: 3.1956\n",
            "--- Epoch 1/5 | Batch 15200/231441 | Loss: 3.2964\n",
            "--- Epoch 1/5 | Batch 15250/231441 | Loss: 3.3656\n",
            "--- Epoch 1/5 | Batch 15300/231441 | Loss: 3.3737\n",
            "--- Epoch 1/5 | Batch 15350/231441 | Loss: 3.2313\n",
            "--- Epoch 1/5 | Batch 15400/231441 | Loss: 3.2949\n",
            "--- Epoch 1/5 | Batch 15450/231441 | Loss: 3.3102\n",
            "--- Epoch 1/5 | Batch 15500/231441 | Loss: 3.1672\n",
            "--- Epoch 1/5 | Batch 15550/231441 | Loss: 3.3691\n",
            "--- Epoch 1/5 | Batch 15600/231441 | Loss: 3.1640\n",
            "--- Epoch 1/5 | Batch 15650/231441 | Loss: 3.3017\n",
            "--- Epoch 1/5 | Batch 15700/231441 | Loss: 3.5083\n",
            "--- Epoch 1/5 | Batch 15750/231441 | Loss: 3.2513\n",
            "--- Epoch 1/5 | Batch 15800/231441 | Loss: 3.3153\n",
            "--- Epoch 1/5 | Batch 15850/231441 | Loss: 3.2878\n",
            "--- Epoch 1/5 | Batch 15900/231441 | Loss: 3.4682\n",
            "--- Epoch 1/5 | Batch 15950/231441 | Loss: 3.2604\n",
            "--- Epoch 1/5 | Batch 16000/231441 | Loss: 3.4115\n",
            "--- Epoch 1/5 | Batch 16050/231441 | Loss: 3.2854\n",
            "--- Epoch 1/5 | Batch 16100/231441 | Loss: 3.2501\n",
            "--- Epoch 1/5 | Batch 16150/231441 | Loss: 3.2434\n",
            "--- Epoch 1/5 | Batch 16200/231441 | Loss: 3.3425\n",
            "--- Epoch 1/5 | Batch 16250/231441 | Loss: 3.2615\n",
            "--- Epoch 1/5 | Batch 16300/231441 | Loss: 3.2299\n",
            "--- Epoch 1/5 | Batch 16350/231441 | Loss: 3.1288\n",
            "--- Epoch 1/5 | Batch 16400/231441 | Loss: 3.2389\n",
            "--- Epoch 1/5 | Batch 16450/231441 | Loss: 3.4186\n",
            "--- Epoch 1/5 | Batch 16500/231441 | Loss: 3.3150\n",
            "--- Epoch 1/5 | Batch 16550/231441 | Loss: 3.1552\n",
            "--- Epoch 1/5 | Batch 16600/231441 | Loss: 3.2451\n",
            "--- Epoch 1/5 | Batch 16650/231441 | Loss: 3.3944\n",
            "--- Epoch 1/5 | Batch 16700/231441 | Loss: 3.2137\n",
            "--- Epoch 1/5 | Batch 16750/231441 | Loss: 3.3148\n",
            "--- Epoch 1/5 | Batch 16800/231441 | Loss: 3.2913\n",
            "--- Epoch 1/5 | Batch 16850/231441 | Loss: 3.1940\n",
            "--- Epoch 1/5 | Batch 16900/231441 | Loss: 3.2372\n",
            "--- Epoch 1/5 | Batch 16950/231441 | Loss: 3.3037\n",
            "--- Epoch 1/5 | Batch 17000/231441 | Loss: 3.2761\n",
            "--- Epoch 1/5 | Batch 17050/231441 | Loss: 3.1075\n",
            "--- Epoch 1/5 | Batch 17100/231441 | Loss: 3.2747\n",
            "--- Epoch 1/5 | Batch 17150/231441 | Loss: 3.0808\n",
            "--- Epoch 1/5 | Batch 17200/231441 | Loss: 3.4604\n",
            "--- Epoch 1/5 | Batch 17250/231441 | Loss: 3.1743\n",
            "--- Epoch 1/5 | Batch 17300/231441 | Loss: 3.1460\n",
            "--- Epoch 1/5 | Batch 17350/231441 | Loss: 3.2464\n",
            "--- Epoch 1/5 | Batch 17400/231441 | Loss: 3.1770\n",
            "--- Epoch 1/5 | Batch 17450/231441 | Loss: 3.2020\n",
            "--- Epoch 1/5 | Batch 17500/231441 | Loss: 3.1696\n",
            "--- Epoch 1/5 | Batch 17550/231441 | Loss: 3.3199\n",
            "--- Epoch 1/5 | Batch 17600/231441 | Loss: 3.3026\n",
            "--- Epoch 1/5 | Batch 17650/231441 | Loss: 3.2294\n",
            "--- Epoch 1/5 | Batch 17700/231441 | Loss: 3.4088\n",
            "--- Epoch 1/5 | Batch 17750/231441 | Loss: 3.1492\n",
            "--- Epoch 1/5 | Batch 17800/231441 | Loss: 3.3154\n",
            "--- Epoch 1/5 | Batch 17850/231441 | Loss: 3.2286\n",
            "--- Epoch 1/5 | Batch 17900/231441 | Loss: 3.1473\n",
            "--- Epoch 1/5 | Batch 17950/231441 | Loss: 3.2555\n",
            "--- Epoch 1/5 | Batch 18000/231441 | Loss: 3.2234\n",
            "--- Epoch 1/5 | Batch 18050/231441 | Loss: 3.0423\n",
            "--- Epoch 1/5 | Batch 18100/231441 | Loss: 3.1854\n",
            "--- Epoch 1/5 | Batch 18150/231441 | Loss: 3.3031\n",
            "--- Epoch 1/5 | Batch 18200/231441 | Loss: 3.1836\n",
            "--- Epoch 1/5 | Batch 18250/231441 | Loss: 3.1622\n",
            "--- Epoch 1/5 | Batch 18300/231441 | Loss: 3.2232\n",
            "--- Epoch 1/5 | Batch 18350/231441 | Loss: 3.1875\n",
            "--- Epoch 1/5 | Batch 18400/231441 | Loss: 3.1824\n",
            "--- Epoch 1/5 | Batch 18450/231441 | Loss: 3.1558\n",
            "--- Epoch 1/5 | Batch 18500/231441 | Loss: 3.1460\n",
            "--- Epoch 1/5 | Batch 18550/231441 | Loss: 3.2534\n",
            "--- Epoch 1/5 | Batch 18600/231441 | Loss: 3.0413\n",
            "--- Epoch 1/5 | Batch 18650/231441 | Loss: 3.0638\n",
            "--- Epoch 1/5 | Batch 18700/231441 | Loss: 3.1826\n",
            "--- Epoch 1/5 | Batch 18750/231441 | Loss: 3.0850\n",
            "--- Epoch 1/5 | Batch 18800/231441 | Loss: 3.2968\n",
            "--- Epoch 1/5 | Batch 18850/231441 | Loss: 3.1764\n",
            "--- Epoch 1/5 | Batch 18900/231441 | Loss: 3.2615\n",
            "--- Epoch 1/5 | Batch 18950/231441 | Loss: 3.2020\n",
            "--- Epoch 1/5 | Batch 19000/231441 | Loss: 3.1340\n",
            "--- Epoch 1/5 | Batch 19050/231441 | Loss: 3.1983\n",
            "--- Epoch 1/5 | Batch 19100/231441 | Loss: 3.0230\n",
            "--- Epoch 1/5 | Batch 19150/231441 | Loss: 3.0943\n",
            "--- Epoch 1/5 | Batch 19200/231441 | Loss: 3.2121\n",
            "--- Epoch 1/5 | Batch 19250/231441 | Loss: 3.0504\n",
            "--- Epoch 1/5 | Batch 19300/231441 | Loss: 3.0170\n",
            "--- Epoch 1/5 | Batch 19350/231441 | Loss: 3.1850\n",
            "--- Epoch 1/5 | Batch 19400/231441 | Loss: 3.0154\n",
            "--- Epoch 1/5 | Batch 19450/231441 | Loss: 3.0409\n",
            "--- Epoch 1/5 | Batch 19500/231441 | Loss: 2.9722\n",
            "--- Epoch 1/5 | Batch 19550/231441 | Loss: 3.1072\n",
            "--- Epoch 1/5 | Batch 19600/231441 | Loss: 3.0499\n",
            "--- Epoch 1/5 | Batch 19650/231441 | Loss: 2.9825\n",
            "--- Epoch 1/5 | Batch 19700/231441 | Loss: 3.0539\n",
            "--- Epoch 1/5 | Batch 19750/231441 | Loss: 2.8510\n",
            "--- Epoch 1/5 | Batch 19800/231441 | Loss: 3.2349\n",
            "--- Epoch 1/5 | Batch 19850/231441 | Loss: 3.0416\n",
            "--- Epoch 1/5 | Batch 19900/231441 | Loss: 3.1134\n",
            "--- Epoch 1/5 | Batch 19950/231441 | Loss: 2.9171\n",
            "--- Epoch 1/5 | Batch 20000/231441 | Loss: 3.2204\n",
            "--- Epoch 1/5 | Batch 20050/231441 | Loss: 3.0989\n",
            "--- Epoch 1/5 | Batch 20100/231441 | Loss: 3.1186\n",
            "--- Epoch 1/5 | Batch 20150/231441 | Loss: 3.2755\n",
            "--- Epoch 1/5 | Batch 20200/231441 | Loss: 3.0597\n",
            "--- Epoch 1/5 | Batch 20250/231441 | Loss: 3.1133\n",
            "--- Epoch 1/5 | Batch 20300/231441 | Loss: 3.0497\n",
            "--- Epoch 1/5 | Batch 20350/231441 | Loss: 2.9471\n",
            "--- Epoch 1/5 | Batch 20400/231441 | Loss: 2.9282\n",
            "--- Epoch 1/5 | Batch 20450/231441 | Loss: 3.0620\n",
            "--- Epoch 1/5 | Batch 20500/231441 | Loss: 3.3029\n",
            "--- Epoch 1/5 | Batch 20550/231441 | Loss: 3.1481\n",
            "--- Epoch 1/5 | Batch 20600/231441 | Loss: 2.8932\n",
            "--- Epoch 1/5 | Batch 20650/231441 | Loss: 3.1308\n",
            "--- Epoch 1/5 | Batch 20700/231441 | Loss: 3.1394\n",
            "--- Epoch 1/5 | Batch 20750/231441 | Loss: 2.9916\n",
            "--- Epoch 1/5 | Batch 20800/231441 | Loss: 2.9512\n",
            "--- Epoch 1/5 | Batch 20850/231441 | Loss: 3.0299\n",
            "--- Epoch 1/5 | Batch 20900/231441 | Loss: 3.2023\n",
            "--- Epoch 1/5 | Batch 20950/231441 | Loss: 3.1266\n",
            "--- Epoch 1/5 | Batch 21000/231441 | Loss: 3.2630\n",
            "--- Epoch 1/5 | Batch 21050/231441 | Loss: 3.1563\n",
            "--- Epoch 1/5 | Batch 21100/231441 | Loss: 3.1511\n",
            "--- Epoch 1/5 | Batch 21150/231441 | Loss: 3.0049\n",
            "--- Epoch 1/5 | Batch 21200/231441 | Loss: 3.0894\n",
            "--- Epoch 1/5 | Batch 21250/231441 | Loss: 3.0787\n",
            "--- Epoch 1/5 | Batch 21300/231441 | Loss: 3.0322\n",
            "--- Epoch 1/5 | Batch 21350/231441 | Loss: 2.9178\n",
            "--- Epoch 1/5 | Batch 21400/231441 | Loss: 2.9480\n",
            "--- Epoch 1/5 | Batch 21450/231441 | Loss: 3.0532\n",
            "--- Epoch 1/5 | Batch 21500/231441 | Loss: 3.0839\n",
            "--- Epoch 1/5 | Batch 21550/231441 | Loss: 3.1973\n",
            "--- Epoch 1/5 | Batch 21600/231441 | Loss: 3.1122\n",
            "--- Epoch 1/5 | Batch 21650/231441 | Loss: 3.0331\n",
            "--- Epoch 1/5 | Batch 21700/231441 | Loss: 2.9609\n",
            "--- Epoch 1/5 | Batch 21750/231441 | Loss: 2.9048\n",
            "--- Epoch 1/5 | Batch 21800/231441 | Loss: 3.0070\n",
            "--- Epoch 1/5 | Batch 21850/231441 | Loss: 2.9889\n",
            "--- Epoch 1/5 | Batch 21900/231441 | Loss: 3.0847\n",
            "--- Epoch 1/5 | Batch 21950/231441 | Loss: 2.9797\n",
            "--- Epoch 1/5 | Batch 22000/231441 | Loss: 2.9839\n",
            "--- Epoch 1/5 | Batch 22050/231441 | Loss: 3.0756\n",
            "--- Epoch 1/5 | Batch 22100/231441 | Loss: 3.2407\n",
            "--- Epoch 1/5 | Batch 22150/231441 | Loss: 3.1763\n",
            "--- Epoch 1/5 | Batch 22200/231441 | Loss: 2.9922\n",
            "--- Epoch 1/5 | Batch 22250/231441 | Loss: 2.8674\n",
            "--- Epoch 1/5 | Batch 22300/231441 | Loss: 3.0759\n",
            "--- Epoch 1/5 | Batch 22350/231441 | Loss: 2.9853\n",
            "--- Epoch 1/5 | Batch 22400/231441 | Loss: 3.0721\n",
            "--- Epoch 1/5 | Batch 22450/231441 | Loss: 2.8974\n",
            "--- Epoch 1/5 | Batch 22500/231441 | Loss: 3.1278\n",
            "--- Epoch 1/5 | Batch 22550/231441 | Loss: 3.1823\n",
            "--- Epoch 1/5 | Batch 22600/231441 | Loss: 3.1230\n",
            "--- Epoch 1/5 | Batch 22650/231441 | Loss: 3.0601\n",
            "--- Epoch 1/5 | Batch 22700/231441 | Loss: 2.9569\n",
            "--- Epoch 1/5 | Batch 22750/231441 | Loss: 2.8450\n",
            "--- Epoch 1/5 | Batch 22800/231441 | Loss: 2.8894\n",
            "--- Epoch 1/5 | Batch 22850/231441 | Loss: 3.0262\n",
            "--- Epoch 1/5 | Batch 22900/231441 | Loss: 2.9605\n",
            "--- Epoch 1/5 | Batch 22950/231441 | Loss: 3.0200\n",
            "--- Epoch 1/5 | Batch 23000/231441 | Loss: 2.9837\n",
            "--- Epoch 1/5 | Batch 23050/231441 | Loss: 2.8584\n",
            "--- Epoch 1/5 | Batch 23100/231441 | Loss: 3.0714\n",
            "--- Epoch 1/5 | Batch 23150/231441 | Loss: 2.8878\n",
            "--- Epoch 1/5 | Batch 23200/231441 | Loss: 3.0814\n",
            "--- Epoch 1/5 | Batch 23250/231441 | Loss: 3.0660\n",
            "--- Epoch 1/5 | Batch 23300/231441 | Loss: 2.9267\n",
            "--- Epoch 1/5 | Batch 23350/231441 | Loss: 3.0174\n",
            "--- Epoch 1/5 | Batch 23400/231441 | Loss: 3.0567\n",
            "--- Epoch 1/5 | Batch 23450/231441 | Loss: 3.1501\n",
            "--- Epoch 1/5 | Batch 23500/231441 | Loss: 2.8917\n",
            "--- Epoch 1/5 | Batch 23550/231441 | Loss: 3.0427\n",
            "--- Epoch 1/5 | Batch 23600/231441 | Loss: 2.9859\n",
            "--- Epoch 1/5 | Batch 23650/231441 | Loss: 3.0507\n",
            "--- Epoch 1/5 | Batch 23700/231441 | Loss: 3.1280\n",
            "--- Epoch 1/5 | Batch 23750/231441 | Loss: 3.0048\n",
            "--- Epoch 1/5 | Batch 23800/231441 | Loss: 2.8159\n",
            "--- Epoch 1/5 | Batch 23850/231441 | Loss: 2.9235\n",
            "--- Epoch 1/5 | Batch 23900/231441 | Loss: 2.9187\n",
            "--- Epoch 1/5 | Batch 23950/231441 | Loss: 3.0632\n",
            "--- Epoch 1/5 | Batch 24000/231441 | Loss: 2.8773\n",
            "--- Epoch 1/5 | Batch 24050/231441 | Loss: 3.0133\n",
            "--- Epoch 1/5 | Batch 24100/231441 | Loss: 2.9605\n",
            "--- Epoch 1/5 | Batch 24150/231441 | Loss: 3.1404\n",
            "--- Epoch 1/5 | Batch 24200/231441 | Loss: 2.9448\n",
            "--- Epoch 1/5 | Batch 24250/231441 | Loss: 2.8015\n",
            "--- Epoch 1/5 | Batch 24300/231441 | Loss: 2.9882\n",
            "--- Epoch 1/5 | Batch 24350/231441 | Loss: 2.8495\n",
            "--- Epoch 1/5 | Batch 24400/231441 | Loss: 3.0330\n",
            "--- Epoch 1/5 | Batch 24450/231441 | Loss: 2.8514\n",
            "--- Epoch 1/5 | Batch 24500/231441 | Loss: 2.9699\n",
            "--- Epoch 1/5 | Batch 24550/231441 | Loss: 2.9155\n",
            "--- Epoch 1/5 | Batch 24600/231441 | Loss: 3.0591\n",
            "--- Epoch 1/5 | Batch 24650/231441 | Loss: 2.9164\n",
            "--- Epoch 1/5 | Batch 24700/231441 | Loss: 2.9005\n",
            "--- Epoch 1/5 | Batch 24750/231441 | Loss: 2.9974\n",
            "--- Epoch 1/5 | Batch 24800/231441 | Loss: 3.0819\n",
            "--- Epoch 1/5 | Batch 24850/231441 | Loss: 2.8418\n",
            "--- Epoch 1/5 | Batch 24900/231441 | Loss: 2.8996\n",
            "--- Epoch 1/5 | Batch 24950/231441 | Loss: 2.9393\n",
            "--- Epoch 1/5 | Batch 25000/231441 | Loss: 3.0027\n",
            "--- Epoch 1/5 | Batch 25050/231441 | Loss: 2.8573\n",
            "--- Epoch 1/5 | Batch 25100/231441 | Loss: 2.8742\n",
            "--- Epoch 1/5 | Batch 25150/231441 | Loss: 2.9855\n",
            "--- Epoch 1/5 | Batch 25200/231441 | Loss: 2.9237\n",
            "--- Epoch 1/5 | Batch 25250/231441 | Loss: 3.1496\n",
            "--- Epoch 1/5 | Batch 25300/231441 | Loss: 2.7952\n",
            "--- Epoch 1/5 | Batch 25350/231441 | Loss: 2.7502\n",
            "--- Epoch 1/5 | Batch 25400/231441 | Loss: 2.7594\n",
            "--- Epoch 1/5 | Batch 25450/231441 | Loss: 2.9197\n",
            "--- Epoch 1/5 | Batch 25500/231441 | Loss: 2.7471\n",
            "--- Epoch 1/5 | Batch 25550/231441 | Loss: 3.1150\n",
            "--- Epoch 1/5 | Batch 25600/231441 | Loss: 3.0218\n",
            "--- Epoch 1/5 | Batch 25650/231441 | Loss: 2.8779\n",
            "--- Epoch 1/5 | Batch 25700/231441 | Loss: 2.9421\n",
            "--- Epoch 1/5 | Batch 25750/231441 | Loss: 2.8981\n",
            "--- Epoch 1/5 | Batch 25800/231441 | Loss: 2.9458\n",
            "--- Epoch 1/5 | Batch 25850/231441 | Loss: 2.8056\n",
            "--- Epoch 1/5 | Batch 25900/231441 | Loss: 2.7697\n",
            "--- Epoch 1/5 | Batch 25950/231441 | Loss: 3.1575\n",
            "--- Epoch 1/5 | Batch 26000/231441 | Loss: 2.8012\n",
            "--- Epoch 1/5 | Batch 26050/231441 | Loss: 2.9249\n",
            "--- Epoch 1/5 | Batch 26100/231441 | Loss: 2.9148\n",
            "--- Epoch 1/5 | Batch 26150/231441 | Loss: 2.9392\n",
            "--- Epoch 1/5 | Batch 26200/231441 | Loss: 2.7189\n",
            "--- Epoch 1/5 | Batch 26250/231441 | Loss: 2.8086\n",
            "--- Epoch 1/5 | Batch 26300/231441 | Loss: 2.8291\n",
            "--- Epoch 1/5 | Batch 26350/231441 | Loss: 2.9090\n",
            "--- Epoch 1/5 | Batch 26400/231441 | Loss: 2.8289\n",
            "--- Epoch 1/5 | Batch 26450/231441 | Loss: 2.9044\n",
            "--- Epoch 1/5 | Batch 26500/231441 | Loss: 2.8703\n",
            "--- Epoch 1/5 | Batch 26550/231441 | Loss: 3.0010\n",
            "--- Epoch 1/5 | Batch 26600/231441 | Loss: 2.8557\n",
            "--- Epoch 1/5 | Batch 26650/231441 | Loss: 2.8590\n",
            "--- Epoch 1/5 | Batch 26700/231441 | Loss: 2.9670\n",
            "--- Epoch 1/5 | Batch 26750/231441 | Loss: 2.9105\n",
            "--- Epoch 1/5 | Batch 26800/231441 | Loss: 3.0195\n",
            "--- Epoch 1/5 | Batch 26850/231441 | Loss: 2.7728\n",
            "--- Epoch 1/5 | Batch 26900/231441 | Loss: 2.9020\n",
            "--- Epoch 1/5 | Batch 26950/231441 | Loss: 2.8982\n",
            "--- Epoch 1/5 | Batch 27000/231441 | Loss: 2.7493\n",
            "--- Epoch 1/5 | Batch 27050/231441 | Loss: 2.8865\n",
            "--- Epoch 1/5 | Batch 27100/231441 | Loss: 2.7627\n",
            "--- Epoch 1/5 | Batch 27150/231441 | Loss: 2.8046\n",
            "--- Epoch 1/5 | Batch 27200/231441 | Loss: 2.8548\n",
            "--- Epoch 1/5 | Batch 27250/231441 | Loss: 2.9184\n",
            "--- Epoch 1/5 | Batch 27300/231441 | Loss: 2.8435\n",
            "--- Epoch 1/5 | Batch 27350/231441 | Loss: 2.9884\n",
            "--- Epoch 1/5 | Batch 27400/231441 | Loss: 2.8445\n",
            "--- Epoch 1/5 | Batch 27450/231441 | Loss: 3.0272\n",
            "--- Epoch 1/5 | Batch 27500/231441 | Loss: 2.7544\n",
            "--- Epoch 1/5 | Batch 27550/231441 | Loss: 2.9103\n",
            "--- Epoch 1/5 | Batch 27600/231441 | Loss: 2.9013\n",
            "--- Epoch 1/5 | Batch 27650/231441 | Loss: 2.7569\n",
            "--- Epoch 1/5 | Batch 27700/231441 | Loss: 2.8215\n",
            "--- Epoch 1/5 | Batch 27750/231441 | Loss: 2.7857\n",
            "--- Epoch 1/5 | Batch 27800/231441 | Loss: 3.0482\n",
            "--- Epoch 1/5 | Batch 27850/231441 | Loss: 3.0166\n",
            "--- Epoch 1/5 | Batch 27900/231441 | Loss: 2.6981\n",
            "--- Epoch 1/5 | Batch 27950/231441 | Loss: 2.8214\n",
            "--- Epoch 1/5 | Batch 28000/231441 | Loss: 2.7698\n",
            "--- Epoch 1/5 | Batch 28050/231441 | Loss: 2.7202\n",
            "--- Epoch 1/5 | Batch 28100/231441 | Loss: 3.0548\n",
            "--- Epoch 1/5 | Batch 28150/231441 | Loss: 2.8962\n",
            "--- Epoch 1/5 | Batch 28200/231441 | Loss: 2.7754\n",
            "--- Epoch 1/5 | Batch 28250/231441 | Loss: 2.6653\n",
            "--- Epoch 1/5 | Batch 28300/231441 | Loss: 2.9121\n",
            "--- Epoch 1/5 | Batch 28350/231441 | Loss: 2.8569\n",
            "--- Epoch 1/5 | Batch 28400/231441 | Loss: 2.7894\n",
            "--- Epoch 1/5 | Batch 28450/231441 | Loss: 2.7531\n",
            "--- Epoch 1/5 | Batch 28500/231441 | Loss: 2.8935\n",
            "--- Epoch 1/5 | Batch 28550/231441 | Loss: 2.9887\n",
            "--- Epoch 1/5 | Batch 28600/231441 | Loss: 3.0371\n",
            "--- Epoch 1/5 | Batch 28650/231441 | Loss: 2.6651\n",
            "--- Epoch 1/5 | Batch 28700/231441 | Loss: 2.7491\n",
            "--- Epoch 1/5 | Batch 28750/231441 | Loss: 2.7257\n",
            "--- Epoch 1/5 | Batch 28800/231441 | Loss: 2.9222\n",
            "--- Epoch 1/5 | Batch 28850/231441 | Loss: 2.7143\n",
            "--- Epoch 1/5 | Batch 28900/231441 | Loss: 2.8955\n",
            "--- Epoch 1/5 | Batch 28950/231441 | Loss: 2.7558\n",
            "--- Epoch 1/5 | Batch 29000/231441 | Loss: 2.8351\n",
            "--- Epoch 1/5 | Batch 29050/231441 | Loss: 2.7821\n",
            "--- Epoch 1/5 | Batch 29100/231441 | Loss: 2.8066\n",
            "--- Epoch 1/5 | Batch 29150/231441 | Loss: 2.7498\n",
            "--- Epoch 1/5 | Batch 29200/231441 | Loss: 2.7971\n",
            "--- Epoch 1/5 | Batch 29250/231441 | Loss: 2.8654\n",
            "--- Epoch 1/5 | Batch 29300/231441 | Loss: 2.6890\n",
            "--- Epoch 1/5 | Batch 29350/231441 | Loss: 2.6888\n",
            "--- Epoch 1/5 | Batch 29400/231441 | Loss: 3.0217\n",
            "--- Epoch 1/5 | Batch 29450/231441 | Loss: 2.9575\n",
            "--- Epoch 1/5 | Batch 29500/231441 | Loss: 2.7989\n",
            "--- Epoch 1/5 | Batch 29550/231441 | Loss: 2.7289\n",
            "--- Epoch 1/5 | Batch 29600/231441 | Loss: 2.8810\n",
            "--- Epoch 1/5 | Batch 29650/231441 | Loss: 2.9179\n",
            "--- Epoch 1/5 | Batch 29700/231441 | Loss: 2.9727\n",
            "--- Epoch 1/5 | Batch 29750/231441 | Loss: 2.9226\n",
            "--- Epoch 1/5 | Batch 29800/231441 | Loss: 2.9727\n",
            "--- Epoch 1/5 | Batch 29850/231441 | Loss: 2.7668\n",
            "--- Epoch 1/5 | Batch 29900/231441 | Loss: 2.6364\n",
            "--- Epoch 1/5 | Batch 29950/231441 | Loss: 2.9494\n",
            "--- Epoch 1/5 | Batch 30000/231441 | Loss: 2.6445\n",
            "--- Epoch 1/5 | Batch 30050/231441 | Loss: 2.7841\n",
            "--- Epoch 1/5 | Batch 30100/231441 | Loss: 2.7681\n",
            "--- Epoch 1/5 | Batch 30150/231441 | Loss: 2.6034\n",
            "--- Epoch 1/5 | Batch 30200/231441 | Loss: 2.7149\n",
            "--- Epoch 1/5 | Batch 30250/231441 | Loss: 2.8149\n",
            "--- Epoch 1/5 | Batch 30300/231441 | Loss: 2.7082\n",
            "--- Epoch 1/5 | Batch 30350/231441 | Loss: 2.8097\n",
            "--- Epoch 1/5 | Batch 30400/231441 | Loss: 2.9613\n",
            "--- Epoch 1/5 | Batch 30450/231441 | Loss: 2.9518\n",
            "--- Epoch 1/5 | Batch 30500/231441 | Loss: 2.7760\n",
            "--- Epoch 1/5 | Batch 30550/231441 | Loss: 2.7366\n",
            "--- Epoch 1/5 | Batch 30600/231441 | Loss: 2.6033\n",
            "--- Epoch 1/5 | Batch 30650/231441 | Loss: 2.6457\n",
            "--- Epoch 1/5 | Batch 30700/231441 | Loss: 2.9316\n",
            "--- Epoch 1/5 | Batch 30750/231441 | Loss: 2.7148\n",
            "--- Epoch 1/5 | Batch 30800/231441 | Loss: 2.9522\n",
            "--- Epoch 1/5 | Batch 30850/231441 | Loss: 2.9023\n",
            "--- Epoch 1/5 | Batch 30900/231441 | Loss: 2.5811\n",
            "--- Epoch 1/5 | Batch 30950/231441 | Loss: 2.7975\n",
            "--- Epoch 1/5 | Batch 31000/231441 | Loss: 2.9722\n",
            "--- Epoch 1/5 | Batch 31050/231441 | Loss: 2.8603\n",
            "--- Epoch 1/5 | Batch 31100/231441 | Loss: 2.8290\n",
            "--- Epoch 1/5 | Batch 31150/231441 | Loss: 2.7781\n",
            "--- Epoch 1/5 | Batch 31200/231441 | Loss: 2.8176\n",
            "--- Epoch 1/5 | Batch 31250/231441 | Loss: 2.8224\n",
            "--- Epoch 1/5 | Batch 31300/231441 | Loss: 2.8027\n",
            "--- Epoch 1/5 | Batch 31350/231441 | Loss: 2.7430\n",
            "--- Epoch 1/5 | Batch 31400/231441 | Loss: 2.7144\n",
            "--- Epoch 1/5 | Batch 31450/231441 | Loss: 2.8493\n",
            "--- Epoch 1/5 | Batch 31500/231441 | Loss: 2.8563\n",
            "--- Epoch 1/5 | Batch 31550/231441 | Loss: 2.7754\n",
            "--- Epoch 1/5 | Batch 31600/231441 | Loss: 2.9099\n",
            "--- Epoch 1/5 | Batch 31650/231441 | Loss: 2.8268\n",
            "--- Epoch 1/5 | Batch 31700/231441 | Loss: 2.7906\n",
            "--- Epoch 1/5 | Batch 31750/231441 | Loss: 2.7916\n",
            "--- Epoch 1/5 | Batch 31800/231441 | Loss: 2.8337\n",
            "--- Epoch 1/5 | Batch 31850/231441 | Loss: 2.7816\n",
            "--- Epoch 1/5 | Batch 31900/231441 | Loss: 2.6898\n",
            "--- Epoch 1/5 | Batch 31950/231441 | Loss: 2.8636\n",
            "--- Epoch 1/5 | Batch 32000/231441 | Loss: 2.7444\n",
            "--- Epoch 1/5 | Batch 32050/231441 | Loss: 2.7494\n",
            "--- Epoch 1/5 | Batch 32100/231441 | Loss: 2.6591\n",
            "--- Epoch 1/5 | Batch 32150/231441 | Loss: 2.8396\n",
            "--- Epoch 1/5 | Batch 32200/231441 | Loss: 2.7209\n",
            "--- Epoch 1/5 | Batch 32250/231441 | Loss: 2.6322\n",
            "--- Epoch 1/5 | Batch 32300/231441 | Loss: 2.8939\n",
            "--- Epoch 1/5 | Batch 32350/231441 | Loss: 2.7464\n",
            "--- Epoch 1/5 | Batch 32400/231441 | Loss: 2.6466\n",
            "--- Epoch 1/5 | Batch 32450/231441 | Loss: 2.7168\n",
            "--- Epoch 1/5 | Batch 32500/231441 | Loss: 2.5790\n",
            "--- Epoch 1/5 | Batch 32550/231441 | Loss: 2.6855\n",
            "--- Epoch 1/5 | Batch 32600/231441 | Loss: 2.6212\n",
            "--- Epoch 1/5 | Batch 32650/231441 | Loss: 2.5776\n",
            "--- Epoch 1/5 | Batch 32700/231441 | Loss: 2.7300\n",
            "--- Epoch 1/5 | Batch 32750/231441 | Loss: 2.8739\n",
            "--- Epoch 1/5 | Batch 32800/231441 | Loss: 2.8978\n",
            "--- Epoch 1/5 | Batch 32850/231441 | Loss: 2.6454\n",
            "--- Epoch 1/5 | Batch 32900/231441 | Loss: 2.8002\n",
            "--- Epoch 1/5 | Batch 32950/231441 | Loss: 2.6249\n",
            "--- Epoch 1/5 | Batch 33000/231441 | Loss: 2.7636\n",
            "--- Epoch 1/5 | Batch 33050/231441 | Loss: 2.6826\n",
            "--- Epoch 1/5 | Batch 33100/231441 | Loss: 2.8014\n",
            "--- Epoch 1/5 | Batch 33150/231441 | Loss: 3.0012\n",
            "--- Epoch 1/5 | Batch 33200/231441 | Loss: 2.5634\n",
            "--- Epoch 1/5 | Batch 33250/231441 | Loss: 2.6670\n",
            "--- Epoch 1/5 | Batch 33300/231441 | Loss: 2.7273\n",
            "--- Epoch 1/5 | Batch 33350/231441 | Loss: 2.7270\n",
            "--- Epoch 1/5 | Batch 33400/231441 | Loss: 2.7544\n",
            "--- Epoch 1/5 | Batch 33450/231441 | Loss: 2.5710\n",
            "--- Epoch 1/5 | Batch 33500/231441 | Loss: 2.7022\n",
            "--- Epoch 1/5 | Batch 33550/231441 | Loss: 2.8072\n",
            "--- Epoch 1/5 | Batch 33600/231441 | Loss: 2.8035\n",
            "--- Epoch 1/5 | Batch 33650/231441 | Loss: 2.5683\n",
            "--- Epoch 1/5 | Batch 33700/231441 | Loss: 2.6502\n",
            "--- Epoch 1/5 | Batch 33750/231441 | Loss: 2.8516\n",
            "--- Epoch 1/5 | Batch 33800/231441 | Loss: 2.6700\n",
            "--- Epoch 1/5 | Batch 33850/231441 | Loss: 2.8737\n",
            "--- Epoch 1/5 | Batch 33900/231441 | Loss: 2.7019\n",
            "--- Epoch 1/5 | Batch 33950/231441 | Loss: 2.6002\n",
            "--- Epoch 1/5 | Batch 34000/231441 | Loss: 2.9466\n",
            "--- Epoch 1/5 | Batch 34050/231441 | Loss: 2.8016\n",
            "--- Epoch 1/5 | Batch 34100/231441 | Loss: 2.7686\n",
            "--- Epoch 1/5 | Batch 34150/231441 | Loss: 2.6221\n",
            "--- Epoch 1/5 | Batch 34200/231441 | Loss: 2.6643\n",
            "--- Epoch 1/5 | Batch 34250/231441 | Loss: 2.6086\n",
            "--- Epoch 1/5 | Batch 34300/231441 | Loss: 2.6662\n",
            "--- Epoch 1/5 | Batch 34350/231441 | Loss: 2.6160\n",
            "--- Epoch 1/5 | Batch 34400/231441 | Loss: 2.5160\n",
            "--- Epoch 1/5 | Batch 34450/231441 | Loss: 2.5458\n",
            "--- Epoch 1/5 | Batch 34500/231441 | Loss: 2.6823\n",
            "--- Epoch 1/5 | Batch 34550/231441 | Loss: 2.6563\n",
            "--- Epoch 1/5 | Batch 34600/231441 | Loss: 2.5605\n",
            "--- Epoch 1/5 | Batch 34650/231441 | Loss: 2.6793\n",
            "--- Epoch 1/5 | Batch 34700/231441 | Loss: 2.5185\n",
            "--- Epoch 1/5 | Batch 34750/231441 | Loss: 2.6847\n",
            "--- Epoch 1/5 | Batch 34800/231441 | Loss: 2.6159\n",
            "--- Epoch 1/5 | Batch 34850/231441 | Loss: 2.5690\n",
            "--- Epoch 1/5 | Batch 34900/231441 | Loss: 2.6389\n",
            "--- Epoch 1/5 | Batch 34950/231441 | Loss: 2.7825\n",
            "--- Epoch 1/5 | Batch 35000/231441 | Loss: 2.6944\n",
            "--- Epoch 1/5 | Batch 35050/231441 | Loss: 2.5681\n",
            "--- Epoch 1/5 | Batch 35100/231441 | Loss: 2.6824\n",
            "--- Epoch 1/5 | Batch 35150/231441 | Loss: 2.5600\n",
            "--- Epoch 1/5 | Batch 35200/231441 | Loss: 2.7546\n",
            "--- Epoch 1/5 | Batch 35250/231441 | Loss: 2.7727\n",
            "--- Epoch 1/5 | Batch 35300/231441 | Loss: 2.6468\n",
            "--- Epoch 1/5 | Batch 35350/231441 | Loss: 2.6098\n",
            "--- Epoch 1/5 | Batch 35400/231441 | Loss: 2.6049\n",
            "--- Epoch 1/5 | Batch 35450/231441 | Loss: 2.7219\n",
            "--- Epoch 1/5 | Batch 35500/231441 | Loss: 2.7204\n",
            "--- Epoch 1/5 | Batch 35550/231441 | Loss: 2.6835\n",
            "--- Epoch 1/5 | Batch 35600/231441 | Loss: 2.6519\n",
            "--- Epoch 1/5 | Batch 35650/231441 | Loss: 2.8975\n",
            "--- Epoch 1/5 | Batch 35700/231441 | Loss: 2.8089\n",
            "--- Epoch 1/5 | Batch 35750/231441 | Loss: 2.9682\n",
            "--- Epoch 1/5 | Batch 35800/231441 | Loss: 2.8100\n",
            "--- Epoch 1/5 | Batch 35850/231441 | Loss: 2.5913\n",
            "--- Epoch 1/5 | Batch 35900/231441 | Loss: 2.6107\n",
            "--- Epoch 1/5 | Batch 35950/231441 | Loss: 2.6832\n",
            "--- Epoch 1/5 | Batch 36000/231441 | Loss: 2.6934\n",
            "--- Epoch 1/5 | Batch 36050/231441 | Loss: 2.5635\n",
            "--- Epoch 1/5 | Batch 36100/231441 | Loss: 2.8176\n",
            "--- Epoch 1/5 | Batch 36150/231441 | Loss: 2.6452\n",
            "--- Epoch 1/5 | Batch 36200/231441 | Loss: 2.5384\n",
            "--- Epoch 1/5 | Batch 36250/231441 | Loss: 2.6947\n",
            "--- Epoch 1/5 | Batch 36300/231441 | Loss: 2.7911\n",
            "--- Epoch 1/5 | Batch 36350/231441 | Loss: 2.6651\n",
            "--- Epoch 1/5 | Batch 36400/231441 | Loss: 2.6323\n",
            "--- Epoch 1/5 | Batch 36450/231441 | Loss: 2.7244\n",
            "--- Epoch 1/5 | Batch 36500/231441 | Loss: 2.7241\n",
            "--- Epoch 1/5 | Batch 36550/231441 | Loss: 2.6096\n",
            "--- Epoch 1/5 | Batch 36600/231441 | Loss: 2.7044\n",
            "--- Epoch 1/5 | Batch 36650/231441 | Loss: 2.7723\n",
            "--- Epoch 1/5 | Batch 36700/231441 | Loss: 2.7345\n",
            "--- Epoch 1/5 | Batch 36750/231441 | Loss: 2.5931\n",
            "--- Epoch 1/5 | Batch 36800/231441 | Loss: 2.5537\n",
            "--- Epoch 1/5 | Batch 36850/231441 | Loss: 2.6694\n",
            "--- Epoch 1/5 | Batch 36900/231441 | Loss: 2.6830\n",
            "--- Epoch 1/5 | Batch 36950/231441 | Loss: 2.6428\n",
            "--- Epoch 1/5 | Batch 37000/231441 | Loss: 2.6346\n",
            "--- Epoch 1/5 | Batch 37050/231441 | Loss: 2.6707\n",
            "--- Epoch 1/5 | Batch 37100/231441 | Loss: 2.5805\n",
            "--- Epoch 1/5 | Batch 37150/231441 | Loss: 2.7290\n",
            "--- Epoch 1/5 | Batch 37200/231441 | Loss: 2.6140\n",
            "--- Epoch 1/5 | Batch 37250/231441 | Loss: 2.6842\n",
            "--- Epoch 1/5 | Batch 37300/231441 | Loss: 2.6332\n",
            "--- Epoch 1/5 | Batch 37350/231441 | Loss: 2.7763\n",
            "--- Epoch 1/5 | Batch 37400/231441 | Loss: 2.5297\n",
            "--- Epoch 1/5 | Batch 37450/231441 | Loss: 2.6677\n",
            "--- Epoch 1/5 | Batch 37500/231441 | Loss: 2.8625\n",
            "--- Epoch 1/5 | Batch 37550/231441 | Loss: 2.7657\n",
            "--- Epoch 1/5 | Batch 37600/231441 | Loss: 2.4871\n",
            "--- Epoch 1/5 | Batch 37650/231441 | Loss: 2.5745\n",
            "--- Epoch 1/5 | Batch 37700/231441 | Loss: 2.7133\n",
            "--- Epoch 1/5 | Batch 37750/231441 | Loss: 2.7224\n",
            "--- Epoch 1/5 | Batch 37800/231441 | Loss: 2.5507\n",
            "--- Epoch 1/5 | Batch 37850/231441 | Loss: 2.6117\n",
            "--- Epoch 1/5 | Batch 37900/231441 | Loss: 2.5473\n",
            "--- Epoch 1/5 | Batch 37950/231441 | Loss: 2.5791\n",
            "--- Epoch 1/5 | Batch 38000/231441 | Loss: 2.4608\n",
            "--- Epoch 1/5 | Batch 38050/231441 | Loss: 2.7740\n",
            "--- Epoch 1/5 | Batch 38100/231441 | Loss: 2.7518\n",
            "--- Epoch 1/5 | Batch 38150/231441 | Loss: 2.6883\n",
            "--- Epoch 1/5 | Batch 38200/231441 | Loss: 2.7776\n",
            "--- Epoch 1/5 | Batch 38250/231441 | Loss: 2.7564\n",
            "--- Epoch 1/5 | Batch 38300/231441 | Loss: 2.5517\n",
            "--- Epoch 1/5 | Batch 38350/231441 | Loss: 2.6214\n",
            "--- Epoch 1/5 | Batch 38400/231441 | Loss: 2.7110\n",
            "--- Epoch 1/5 | Batch 38450/231441 | Loss: 2.6062\n",
            "--- Epoch 1/5 | Batch 38500/231441 | Loss: 2.5915\n",
            "--- Epoch 1/5 | Batch 38550/231441 | Loss: 2.6561\n",
            "--- Epoch 1/5 | Batch 38600/231441 | Loss: 2.4997\n",
            "--- Epoch 1/5 | Batch 38650/231441 | Loss: 2.5028\n",
            "--- Epoch 1/5 | Batch 38700/231441 | Loss: 2.5866\n",
            "--- Epoch 1/5 | Batch 38750/231441 | Loss: 2.5987\n",
            "--- Epoch 1/5 | Batch 38800/231441 | Loss: 2.6806\n",
            "--- Epoch 1/5 | Batch 38850/231441 | Loss: 2.4153\n",
            "--- Epoch 1/5 | Batch 38900/231441 | Loss: 2.6294\n",
            "--- Epoch 1/5 | Batch 38950/231441 | Loss: 2.6098\n",
            "--- Epoch 1/5 | Batch 39000/231441 | Loss: 2.7300\n",
            "--- Epoch 1/5 | Batch 39050/231441 | Loss: 2.6171\n",
            "--- Epoch 1/5 | Batch 39100/231441 | Loss: 2.4522\n",
            "--- Epoch 1/5 | Batch 39150/231441 | Loss: 2.5054\n",
            "--- Epoch 1/5 | Batch 39200/231441 | Loss: 2.6084\n",
            "--- Epoch 1/5 | Batch 39250/231441 | Loss: 2.6268\n",
            "--- Epoch 1/5 | Batch 39300/231441 | Loss: 2.5542\n",
            "--- Epoch 1/5 | Batch 39350/231441 | Loss: 2.6115\n",
            "--- Epoch 1/5 | Batch 39400/231441 | Loss: 2.4563\n",
            "--- Epoch 1/5 | Batch 39450/231441 | Loss: 2.6256\n",
            "--- Epoch 1/5 | Batch 39500/231441 | Loss: 2.4862\n",
            "--- Epoch 1/5 | Batch 39550/231441 | Loss: 2.6443\n",
            "--- Epoch 1/5 | Batch 39600/231441 | Loss: 2.3833\n",
            "--- Epoch 1/5 | Batch 39650/231441 | Loss: 2.4875\n",
            "--- Epoch 1/5 | Batch 39700/231441 | Loss: 2.5206\n",
            "--- Epoch 1/5 | Batch 39750/231441 | Loss: 2.6160\n",
            "--- Epoch 1/5 | Batch 39800/231441 | Loss: 2.8885\n",
            "--- Epoch 1/5 | Batch 39850/231441 | Loss: 2.4473\n",
            "--- Epoch 1/5 | Batch 39900/231441 | Loss: 2.5764\n",
            "--- Epoch 1/5 | Batch 39950/231441 | Loss: 2.4204\n",
            "--- Epoch 1/5 | Batch 40000/231441 | Loss: 2.5875\n",
            "--- Epoch 1/5 | Batch 40050/231441 | Loss: 2.5554\n",
            "--- Epoch 1/5 | Batch 40100/231441 | Loss: 2.6842\n",
            "--- Epoch 1/5 | Batch 40150/231441 | Loss: 2.6516\n",
            "--- Epoch 1/5 | Batch 40200/231441 | Loss: 2.5535\n",
            "--- Epoch 1/5 | Batch 40250/231441 | Loss: 2.4408\n",
            "--- Epoch 1/5 | Batch 40300/231441 | Loss: 2.5509\n",
            "--- Epoch 1/5 | Batch 40350/231441 | Loss: 2.5902\n",
            "--- Epoch 1/5 | Batch 40400/231441 | Loss: 2.4815\n",
            "--- Epoch 1/5 | Batch 40450/231441 | Loss: 2.6692\n",
            "--- Epoch 1/5 | Batch 40500/231441 | Loss: 2.5444\n",
            "--- Epoch 1/5 | Batch 40550/231441 | Loss: 2.6151\n",
            "--- Epoch 1/5 | Batch 40600/231441 | Loss: 2.5343\n",
            "--- Epoch 1/5 | Batch 40650/231441 | Loss: 2.6826\n",
            "--- Epoch 1/5 | Batch 40700/231441 | Loss: 2.5678\n",
            "--- Epoch 1/5 | Batch 40750/231441 | Loss: 2.6188\n",
            "--- Epoch 1/5 | Batch 40800/231441 | Loss: 2.4824\n",
            "--- Epoch 1/5 | Batch 40850/231441 | Loss: 2.4371\n",
            "--- Epoch 1/5 | Batch 40900/231441 | Loss: 2.7650\n",
            "--- Epoch 1/5 | Batch 40950/231441 | Loss: 2.6640\n",
            "--- Epoch 1/5 | Batch 41000/231441 | Loss: 2.7252\n",
            "--- Epoch 1/5 | Batch 41050/231441 | Loss: 2.6974\n",
            "--- Epoch 1/5 | Batch 41100/231441 | Loss: 2.5604\n",
            "--- Epoch 1/5 | Batch 41150/231441 | Loss: 2.5281\n",
            "--- Epoch 1/5 | Batch 41200/231441 | Loss: 2.7111\n",
            "--- Epoch 1/5 | Batch 41250/231441 | Loss: 2.5866\n",
            "--- Epoch 1/5 | Batch 41300/231441 | Loss: 2.6286\n",
            "--- Epoch 1/5 | Batch 41350/231441 | Loss: 2.5520\n",
            "--- Epoch 1/5 | Batch 41400/231441 | Loss: 2.4423\n",
            "--- Epoch 1/5 | Batch 41450/231441 | Loss: 2.5286\n",
            "--- Epoch 1/5 | Batch 41500/231441 | Loss: 2.4094\n",
            "--- Epoch 1/5 | Batch 41550/231441 | Loss: 2.7176\n",
            "--- Epoch 1/5 | Batch 41600/231441 | Loss: 2.6435\n",
            "--- Epoch 1/5 | Batch 41650/231441 | Loss: 2.6440\n",
            "--- Epoch 1/5 | Batch 41700/231441 | Loss: 2.4275\n",
            "--- Epoch 1/5 | Batch 41750/231441 | Loss: 2.8406\n",
            "--- Epoch 1/5 | Batch 41800/231441 | Loss: 2.6045\n",
            "--- Epoch 1/5 | Batch 41850/231441 | Loss: 2.6045\n",
            "--- Epoch 1/5 | Batch 41900/231441 | Loss: 2.5746\n",
            "--- Epoch 1/5 | Batch 41950/231441 | Loss: 2.5446\n",
            "--- Epoch 1/5 | Batch 42000/231441 | Loss: 2.5008\n",
            "--- Epoch 1/5 | Batch 42050/231441 | Loss: 2.4472\n",
            "--- Epoch 1/5 | Batch 42100/231441 | Loss: 2.7285\n",
            "--- Epoch 1/5 | Batch 42150/231441 | Loss: 2.4798\n",
            "--- Epoch 1/5 | Batch 42200/231441 | Loss: 2.4148\n",
            "--- Epoch 1/5 | Batch 42250/231441 | Loss: 2.5888\n",
            "--- Epoch 1/5 | Batch 42300/231441 | Loss: 2.6763\n",
            "--- Epoch 1/5 | Batch 42350/231441 | Loss: 2.5147\n",
            "--- Epoch 1/5 | Batch 42400/231441 | Loss: 2.5746\n",
            "--- Epoch 1/5 | Batch 42450/231441 | Loss: 2.6600\n",
            "--- Epoch 1/5 | Batch 42500/231441 | Loss: 2.5274\n",
            "--- Epoch 1/5 | Batch 42550/231441 | Loss: 2.4784\n",
            "--- Epoch 1/5 | Batch 42600/231441 | Loss: 2.4379\n",
            "--- Epoch 1/5 | Batch 42650/231441 | Loss: 2.4565\n",
            "--- Epoch 1/5 | Batch 42700/231441 | Loss: 2.5468\n",
            "--- Epoch 1/5 | Batch 42750/231441 | Loss: 2.5228\n",
            "--- Epoch 1/5 | Batch 42800/231441 | Loss: 2.6795\n",
            "--- Epoch 1/5 | Batch 42850/231441 | Loss: 2.5041\n",
            "--- Epoch 1/5 | Batch 42900/231441 | Loss: 2.4099\n",
            "--- Epoch 1/5 | Batch 42950/231441 | Loss: 2.5292\n",
            "--- Epoch 1/5 | Batch 43000/231441 | Loss: 2.6583\n",
            "--- Epoch 1/5 | Batch 43050/231441 | Loss: 2.4969\n",
            "--- Epoch 1/5 | Batch 43100/231441 | Loss: 2.5667\n",
            "--- Epoch 1/5 | Batch 43150/231441 | Loss: 2.3978\n",
            "--- Epoch 1/5 | Batch 43200/231441 | Loss: 2.4970\n",
            "--- Epoch 1/5 | Batch 43250/231441 | Loss: 2.4727\n",
            "--- Epoch 1/5 | Batch 43300/231441 | Loss: 2.6340\n",
            "--- Epoch 1/5 | Batch 43350/231441 | Loss: 2.4795\n",
            "--- Epoch 1/5 | Batch 43400/231441 | Loss: 2.5634\n",
            "--- Epoch 1/5 | Batch 43450/231441 | Loss: 2.3578\n",
            "--- Epoch 1/5 | Batch 43500/231441 | Loss: 2.6276\n",
            "--- Epoch 1/5 | Batch 43550/231441 | Loss: 2.7327\n",
            "--- Epoch 1/5 | Batch 43600/231441 | Loss: 2.5604\n",
            "--- Epoch 1/5 | Batch 43650/231441 | Loss: 2.3642\n",
            "--- Epoch 1/5 | Batch 43700/231441 | Loss: 2.5316\n",
            "--- Epoch 1/5 | Batch 43750/231441 | Loss: 2.5459\n",
            "--- Epoch 1/5 | Batch 43800/231441 | Loss: 2.4892\n",
            "--- Epoch 1/5 | Batch 43850/231441 | Loss: 2.4086\n",
            "--- Epoch 1/5 | Batch 43900/231441 | Loss: 2.6242\n",
            "--- Epoch 1/5 | Batch 43950/231441 | Loss: 2.4795\n",
            "--- Epoch 1/5 | Batch 44000/231441 | Loss: 2.4962\n",
            "--- Epoch 1/5 | Batch 44050/231441 | Loss: 2.4006\n",
            "--- Epoch 1/5 | Batch 44100/231441 | Loss: 2.6246\n",
            "--- Epoch 1/5 | Batch 44150/231441 | Loss: 2.6398\n",
            "--- Epoch 1/5 | Batch 44200/231441 | Loss: 2.6028\n",
            "--- Epoch 1/5 | Batch 44250/231441 | Loss: 2.6782\n",
            "--- Epoch 1/5 | Batch 44300/231441 | Loss: 2.6393\n",
            "--- Epoch 1/5 | Batch 44350/231441 | Loss: 2.5769\n",
            "--- Epoch 1/5 | Batch 44400/231441 | Loss: 2.4217\n",
            "--- Epoch 1/5 | Batch 44450/231441 | Loss: 2.5392\n",
            "--- Epoch 1/5 | Batch 44500/231441 | Loss: 2.4176\n",
            "--- Epoch 1/5 | Batch 44550/231441 | Loss: 2.4632\n",
            "--- Epoch 1/5 | Batch 44600/231441 | Loss: 2.5636\n",
            "--- Epoch 1/5 | Batch 44650/231441 | Loss: 2.6137\n",
            "--- Epoch 1/5 | Batch 44700/231441 | Loss: 2.5615\n",
            "--- Epoch 1/5 | Batch 44750/231441 | Loss: 2.6509\n",
            "--- Epoch 1/5 | Batch 44800/231441 | Loss: 2.5488\n",
            "--- Epoch 1/5 | Batch 44850/231441 | Loss: 2.5427\n",
            "--- Epoch 1/5 | Batch 44900/231441 | Loss: 2.7001\n",
            "--- Epoch 1/5 | Batch 44950/231441 | Loss: 2.5828\n",
            "--- Epoch 1/5 | Batch 45000/231441 | Loss: 2.5052\n",
            "--- Epoch 1/5 | Batch 45050/231441 | Loss: 2.2729\n",
            "--- Epoch 1/5 | Batch 45100/231441 | Loss: 2.4969\n",
            "--- Epoch 1/5 | Batch 45150/231441 | Loss: 2.4917\n",
            "--- Epoch 1/5 | Batch 45200/231441 | Loss: 2.4518\n",
            "--- Epoch 1/5 | Batch 45250/231441 | Loss: 2.4888\n",
            "--- Epoch 1/5 | Batch 45300/231441 | Loss: 2.4527\n",
            "--- Epoch 1/5 | Batch 45350/231441 | Loss: 2.5467\n",
            "--- Epoch 1/5 | Batch 45400/231441 | Loss: 2.4976\n",
            "--- Epoch 1/5 | Batch 45450/231441 | Loss: 2.4663\n",
            "--- Epoch 1/5 | Batch 45500/231441 | Loss: 2.5510\n",
            "--- Epoch 1/5 | Batch 45550/231441 | Loss: 2.4677\n",
            "--- Epoch 1/5 | Batch 45600/231441 | Loss: 2.5106\n",
            "--- Epoch 1/5 | Batch 45650/231441 | Loss: 2.4462\n",
            "--- Epoch 1/5 | Batch 45700/231441 | Loss: 2.4029\n",
            "--- Epoch 1/5 | Batch 45750/231441 | Loss: 2.5577\n",
            "--- Epoch 1/5 | Batch 45800/231441 | Loss: 2.4717\n",
            "--- Epoch 1/5 | Batch 45850/231441 | Loss: 2.3430\n",
            "--- Epoch 1/5 | Batch 45900/231441 | Loss: 2.5411\n",
            "--- Epoch 1/5 | Batch 45950/231441 | Loss: 2.4817\n",
            "--- Epoch 1/5 | Batch 46000/231441 | Loss: 2.6567\n",
            "--- Epoch 1/5 | Batch 46050/231441 | Loss: 2.3397\n",
            "--- Epoch 1/5 | Batch 46100/231441 | Loss: 2.7021\n",
            "--- Epoch 1/5 | Batch 46150/231441 | Loss: 2.5885\n",
            "--- Epoch 1/5 | Batch 46200/231441 | Loss: 2.5543\n",
            "--- Epoch 1/5 | Batch 46250/231441 | Loss: 2.4269\n",
            "--- Epoch 1/5 | Batch 46300/231441 | Loss: 2.5656\n",
            "--- Epoch 1/5 | Batch 46350/231441 | Loss: 2.4867\n",
            "--- Epoch 1/5 | Batch 46400/231441 | Loss: 2.5607\n",
            "--- Epoch 1/5 | Batch 46450/231441 | Loss: 2.5085\n",
            "--- Epoch 1/5 | Batch 46500/231441 | Loss: 2.6664\n",
            "--- Epoch 1/5 | Batch 46550/231441 | Loss: 2.7346\n",
            "--- Epoch 1/5 | Batch 46600/231441 | Loss: 2.5242\n",
            "--- Epoch 1/5 | Batch 46650/231441 | Loss: 2.4208\n",
            "--- Epoch 1/5 | Batch 46700/231441 | Loss: 2.3672\n",
            "--- Epoch 1/5 | Batch 46750/231441 | Loss: 2.4077\n",
            "--- Epoch 1/5 | Batch 46800/231441 | Loss: 2.4765\n",
            "--- Epoch 1/5 | Batch 46850/231441 | Loss: 2.5335\n",
            "--- Epoch 1/5 | Batch 46900/231441 | Loss: 2.4973\n",
            "--- Epoch 1/5 | Batch 46950/231441 | Loss: 2.4022\n",
            "--- Epoch 1/5 | Batch 47000/231441 | Loss: 2.5023\n",
            "--- Epoch 1/5 | Batch 47050/231441 | Loss: 2.5632\n",
            "--- Epoch 1/5 | Batch 47100/231441 | Loss: 2.6965\n",
            "--- Epoch 1/5 | Batch 47150/231441 | Loss: 2.5015\n",
            "--- Epoch 1/5 | Batch 47200/231441 | Loss: 2.4385\n",
            "--- Epoch 1/5 | Batch 47250/231441 | Loss: 2.6756\n",
            "--- Epoch 1/5 | Batch 47300/231441 | Loss: 2.6098\n",
            "--- Epoch 1/5 | Batch 47350/231441 | Loss: 2.4103\n",
            "--- Epoch 1/5 | Batch 47400/231441 | Loss: 2.4941\n",
            "--- Epoch 1/5 | Batch 47450/231441 | Loss: 2.4952\n",
            "--- Epoch 1/5 | Batch 47500/231441 | Loss: 2.4140\n",
            "--- Epoch 1/5 | Batch 47550/231441 | Loss: 2.5665\n",
            "--- Epoch 1/5 | Batch 47600/231441 | Loss: 2.4440\n",
            "--- Epoch 1/5 | Batch 47650/231441 | Loss: 2.4845\n",
            "--- Epoch 1/5 | Batch 47700/231441 | Loss: 2.3249\n",
            "--- Epoch 1/5 | Batch 47750/231441 | Loss: 2.6553\n",
            "--- Epoch 1/5 | Batch 47800/231441 | Loss: 2.3670\n",
            "--- Epoch 1/5 | Batch 47850/231441 | Loss: 2.4411\n",
            "--- Epoch 1/5 | Batch 47900/231441 | Loss: 2.6521\n",
            "--- Epoch 1/5 | Batch 47950/231441 | Loss: 2.4719\n",
            "--- Epoch 1/5 | Batch 48000/231441 | Loss: 2.5770\n",
            "--- Epoch 1/5 | Batch 48050/231441 | Loss: 2.5474\n",
            "--- Epoch 1/5 | Batch 48100/231441 | Loss: 2.3917\n",
            "--- Epoch 1/5 | Batch 48150/231441 | Loss: 2.4538\n",
            "--- Epoch 1/5 | Batch 48200/231441 | Loss: 2.4988\n",
            "--- Epoch 1/5 | Batch 48250/231441 | Loss: 2.6856\n",
            "--- Epoch 1/5 | Batch 48300/231441 | Loss: 2.5138\n",
            "--- Epoch 1/5 | Batch 48350/231441 | Loss: 2.5023\n",
            "--- Epoch 1/5 | Batch 48400/231441 | Loss: 2.5455\n",
            "--- Epoch 1/5 | Batch 48450/231441 | Loss: 2.3966\n",
            "--- Epoch 1/5 | Batch 48500/231441 | Loss: 2.6510\n",
            "--- Epoch 1/5 | Batch 48550/231441 | Loss: 2.4786\n",
            "--- Epoch 1/5 | Batch 48600/231441 | Loss: 2.4823\n",
            "--- Epoch 1/5 | Batch 48650/231441 | Loss: 2.4903\n",
            "--- Epoch 1/5 | Batch 48700/231441 | Loss: 2.5820\n",
            "--- Epoch 1/5 | Batch 48750/231441 | Loss: 2.6098\n",
            "--- Epoch 1/5 | Batch 48800/231441 | Loss: 2.5996\n",
            "--- Epoch 1/5 | Batch 48850/231441 | Loss: 2.4049\n",
            "--- Epoch 1/5 | Batch 48900/231441 | Loss: 2.5745\n",
            "--- Epoch 1/5 | Batch 48950/231441 | Loss: 2.5861\n",
            "--- Epoch 1/5 | Batch 49000/231441 | Loss: 2.5687\n",
            "--- Epoch 1/5 | Batch 49050/231441 | Loss: 2.5979\n",
            "--- Epoch 1/5 | Batch 49100/231441 | Loss: 2.5067\n",
            "--- Epoch 1/5 | Batch 49150/231441 | Loss: 2.5331\n",
            "--- Epoch 1/5 | Batch 49200/231441 | Loss: 2.3178\n",
            "--- Epoch 1/5 | Batch 49250/231441 | Loss: 2.3866\n",
            "--- Epoch 1/5 | Batch 49300/231441 | Loss: 2.4155\n",
            "--- Epoch 1/5 | Batch 49350/231441 | Loss: 2.5024\n",
            "--- Epoch 1/5 | Batch 49400/231441 | Loss: 2.4835\n",
            "--- Epoch 1/5 | Batch 49450/231441 | Loss: 2.4927\n",
            "--- Epoch 1/5 | Batch 49500/231441 | Loss: 2.5780\n",
            "--- Epoch 1/5 | Batch 49550/231441 | Loss: 2.4675\n",
            "--- Epoch 1/5 | Batch 49600/231441 | Loss: 2.3984\n",
            "--- Epoch 1/5 | Batch 49650/231441 | Loss: 2.4857\n",
            "--- Epoch 1/5 | Batch 49700/231441 | Loss: 2.4198\n",
            "--- Epoch 1/5 | Batch 49750/231441 | Loss: 2.3729\n",
            "--- Epoch 1/5 | Batch 49800/231441 | Loss: 2.4933\n",
            "--- Epoch 1/5 | Batch 49850/231441 | Loss: 2.4986\n",
            "--- Epoch 1/5 | Batch 49900/231441 | Loss: 2.3547\n",
            "--- Epoch 1/5 | Batch 49950/231441 | Loss: 2.5147\n",
            "--- Epoch 1/5 | Batch 50000/231441 | Loss: 2.3514\n",
            "--- Epoch 1/5 | Batch 50050/231441 | Loss: 2.4223\n",
            "--- Epoch 1/5 | Batch 50100/231441 | Loss: 2.4404\n",
            "--- Epoch 1/5 | Batch 50150/231441 | Loss: 2.5038\n",
            "--- Epoch 1/5 | Batch 50200/231441 | Loss: 2.4822\n",
            "--- Epoch 1/5 | Batch 50250/231441 | Loss: 2.5289\n",
            "--- Epoch 1/5 | Batch 50300/231441 | Loss: 2.4323\n",
            "--- Epoch 1/5 | Batch 50350/231441 | Loss: 2.6070\n",
            "--- Epoch 1/5 | Batch 50400/231441 | Loss: 2.5612\n",
            "--- Epoch 1/5 | Batch 50450/231441 | Loss: 2.2958\n",
            "--- Epoch 1/5 | Batch 50500/231441 | Loss: 2.5259\n",
            "--- Epoch 1/5 | Batch 50550/231441 | Loss: 2.5737\n",
            "--- Epoch 1/5 | Batch 50600/231441 | Loss: 2.5332\n",
            "--- Epoch 1/5 | Batch 50650/231441 | Loss: 2.3475\n",
            "--- Epoch 1/5 | Batch 50700/231441 | Loss: 2.5679\n",
            "--- Epoch 1/5 | Batch 50750/231441 | Loss: 2.3228\n",
            "--- Epoch 1/5 | Batch 50800/231441 | Loss: 2.3069\n",
            "--- Epoch 1/5 | Batch 50850/231441 | Loss: 2.4181\n",
            "--- Epoch 1/5 | Batch 50900/231441 | Loss: 2.5640\n",
            "--- Epoch 1/5 | Batch 50950/231441 | Loss: 2.5259\n",
            "--- Epoch 1/5 | Batch 51000/231441 | Loss: 2.4359\n",
            "--- Epoch 1/5 | Batch 51050/231441 | Loss: 2.5716\n",
            "--- Epoch 1/5 | Batch 51100/231441 | Loss: 2.4412\n",
            "--- Epoch 1/5 | Batch 51150/231441 | Loss: 2.3775\n",
            "--- Epoch 1/5 | Batch 51200/231441 | Loss: 2.5308\n",
            "--- Epoch 1/5 | Batch 51250/231441 | Loss: 2.4911\n",
            "--- Epoch 1/5 | Batch 51300/231441 | Loss: 2.6370\n",
            "--- Epoch 1/5 | Batch 51350/231441 | Loss: 2.4492\n",
            "--- Epoch 1/5 | Batch 51400/231441 | Loss: 2.5179\n",
            "--- Epoch 1/5 | Batch 51450/231441 | Loss: 2.5184\n",
            "--- Epoch 1/5 | Batch 51500/231441 | Loss: 2.5042\n",
            "--- Epoch 1/5 | Batch 51550/231441 | Loss: 2.3519\n",
            "--- Epoch 1/5 | Batch 51600/231441 | Loss: 2.4791\n",
            "--- Epoch 1/5 | Batch 51650/231441 | Loss: 2.6461\n",
            "--- Epoch 1/5 | Batch 51700/231441 | Loss: 2.4433\n",
            "--- Epoch 1/5 | Batch 51750/231441 | Loss: 2.3528\n",
            "--- Epoch 1/5 | Batch 51800/231441 | Loss: 2.4809\n",
            "--- Epoch 1/5 | Batch 51850/231441 | Loss: 2.3685\n",
            "--- Epoch 1/5 | Batch 51900/231441 | Loss: 2.5075\n",
            "--- Epoch 1/5 | Batch 51950/231441 | Loss: 2.3927\n",
            "--- Epoch 1/5 | Batch 52000/231441 | Loss: 2.3094\n",
            "--- Epoch 1/5 | Batch 52050/231441 | Loss: 2.3812\n",
            "--- Epoch 1/5 | Batch 52100/231441 | Loss: 2.4774\n",
            "--- Epoch 1/5 | Batch 52150/231441 | Loss: 2.4737\n",
            "--- Epoch 1/5 | Batch 52200/231441 | Loss: 2.4346\n",
            "--- Epoch 1/5 | Batch 52250/231441 | Loss: 2.5300\n",
            "--- Epoch 1/5 | Batch 52300/231441 | Loss: 2.5687\n",
            "--- Epoch 1/5 | Batch 52350/231441 | Loss: 2.5409\n",
            "--- Epoch 1/5 | Batch 52400/231441 | Loss: 2.4712\n",
            "--- Epoch 1/5 | Batch 52450/231441 | Loss: 2.2700\n",
            "--- Epoch 1/5 | Batch 52500/231441 | Loss: 2.4538\n",
            "--- Epoch 1/5 | Batch 52550/231441 | Loss: 2.4181\n",
            "--- Epoch 1/5 | Batch 52600/231441 | Loss: 2.4723\n",
            "--- Epoch 1/5 | Batch 52650/231441 | Loss: 2.4677\n",
            "--- Epoch 1/5 | Batch 52700/231441 | Loss: 2.4175\n",
            "--- Epoch 1/5 | Batch 52750/231441 | Loss: 2.3466\n",
            "--- Epoch 1/5 | Batch 52800/231441 | Loss: 2.4252\n",
            "--- Epoch 1/5 | Batch 52850/231441 | Loss: 2.6891\n",
            "--- Epoch 1/5 | Batch 52900/231441 | Loss: 2.2762\n",
            "--- Epoch 1/5 | Batch 52950/231441 | Loss: 2.5703\n",
            "--- Epoch 1/5 | Batch 53000/231441 | Loss: 2.2924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-485987950.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1874781150.py\u001b[0m in \u001b[0;36mdo_training\u001b[0;34m(train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mtrain_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0minputs_jax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/array.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_replicated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_addressable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_single_device_sharding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_replicated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPmapSharding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "do_training(train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AdYvgPnHMhjD"
      },
      "id": "AdYvgPnHMhjD"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}